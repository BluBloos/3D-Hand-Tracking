\documentclass[10pt]{article}

\title{Occlusion invariant, dense two-hand tracking using a monocular RGB input}
\author{Noah Cabral \\ Maddie Mackie \\ Lucas Coster \\ Max Vincent \\ Oscar Lu}
\date{}

\begin{document}

    \maketitle
    \section{Abstract}

    The problem is to track the 3D mesh and 3D keypoints of two-hands through high 
    hand-to-hand and hand-to-object contact using just a single monocular RGB camera. 
    In the space of hand-tracking research, this problem remains the most pressing 
    issue in realizing a complete solution. Reaching this point will enable new modes of 
    experience for AR/VR applications, along with the general improvement of 
    human-computer interaction overall. Most notably, improvements would be seen for medical 
    simulations, 
    military simulations, and the transcription of sign-language. Related works in this space 
    demonstrate an impressive number of different architectures. 
    To name a few, models might use neural rendering, GCNs, GANs, etc. For the sake of simplicity 
    and deep understanding, the team is leveraging the MobileHand 
    architecture. This model consists of 3 layers including a pre-trained MobileNetV3 
    layer, an iterative regression module, and a layer implementing MANO. The iterative regression 
    layer is a CNN that estimates 39 camera and mesh parameters 
    by connecting its output back to its input, enabling estimation to be performed 
    many times. The MANO layer is a differentiable hand model that takes a template mesh through a 
    linear blend skinning routine, rotating it by input pose parameters. It also takes in 
    shape parameters that deform the template mesh by scaling principal components of a learned 
    shape space. To date,
    the team is unable to present any formal evaluation metrics as the model is still in 
    development. Our final model will be evaluated on the 3D PCK, AUC, and “mesh error” metrics, 
    where the mesh error is simply the per-vertex MSE. The team has been able to train UNET to 
    estimate the segmentation masks of the RHD dataset,
    as well as write unit tests for the $B_p$ and $B_s$ template vertex pertubations. For CUCAI, 
    the team will present a demo of the model estimating just one hand. 
    After CUCAI, the team will continue to 
    develop the model to work for two hands at a time, 
    potentially exploring architectures more advanced than MANO as needed.
    
\end{document}