{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BluBloos/3D-Hand-Tracking/blob/main/src/HandTracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytgQcZUUwswL"
      },
      "source": [
        "# SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFT4_Ezkg0cx"
      },
      "source": [
        "## Run ONLY when in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_ZBgm__TNlq"
      },
      "outputs": [],
      "source": [
        "!echo \"Initializing github repository\"\n",
        "!ls -la\n",
        "!rm -r .config/\n",
        "!rm -r sample_data/\n",
        "!git clone https://github.com/BluBloos/QMIND2021-2022/ ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7uiZv5Lg0cz"
      },
      "source": [
        "## Always Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee72KKdTPIsZ"
      },
      "outputs": [],
      "source": [
        "# Download updated project from Github.\n",
        "!git pull\n",
        "\n",
        "##### HANDLE DIFFS WHEN RUNNING IN COLAB #####\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "print(\"In Colab:\", IN_COLAB)\n",
        "import sys\n",
        "if (IN_COLAB):\n",
        "  sys.path.insert(1, '/content/src/')\n",
        "##### HANDLE DIFFS WHEN RUNNING IN COLAB #####\n",
        "\n",
        "########### TEST GPU AND RAM OF COLLAB INSTANCE ###########\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n",
        "########### TEST GPU AND RAM OF COLLAB INSTANCE ###########\n",
        "\n",
        "######### EXTERNAL LIBRARIES #########\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, UpSampling2D, MaxPool2D\n",
        "from tensorflow.keras import Model\n",
        "import random\n",
        "from qmindcolors import cstr\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "#NOTE: Good resource. -> https://www.tensorflow.org/tutorials/quickstart/advanced\n",
        "import cv2 # opencv, for image resizing.\n",
        "!pip install chumpy\n",
        "######### EXTERNAL LIBRARIES #########\n",
        "\n",
        "############## HELPER FUNCTIONS ############## \n",
        "# NOTE(Noah): Stole this function from Stackoverflow :)\n",
        "def rgb2gray(rgb):\n",
        "    return np.expand_dims(np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140]), axis=2)\n",
        "def resize(img, size):\n",
        "    return cv2.resize(img, dsize=(size, size), interpolation=cv2.INTER_CUBIC)\n",
        "############## HELPER FUNCTIONS ############## "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pebm8lODx0fu"
      },
      "source": [
        "# DATA LOADING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SDaXTQZuF0y"
      },
      "outputs": [],
      "source": [
        "# NOTE(Noah): gcs code will on work on the colab as this code is Ubuntu specific.\n",
        "# I attempted to install gcsfuse on my macOS machine, but I have a version that is too new.\n",
        "# also, gcsfuse simply does not work on Windows.\n",
        "#\n",
        "# gcsfuse is actually beta software.\n",
        "if IN_COLAB:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "  # we know that we are on an Ubuntu machine.\n",
        "  # Thus, installing gcsfuse will be done via the Ubuntu instructions.\n",
        "  # https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/installing.md#ubuntu-and-debian-latest-releases\n",
        "  !echo \"deb http://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "  !curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "  \n",
        "  # -y in apt-get will assume \"yes\" as the answer to all prompts.\n",
        "  # -q in apt-get will make things \"quiet\" for us. Nice!\n",
        "  !sudo apt-get -y -q update\n",
        "  !sudo apt-get -y -q install gcsfuse\n",
        "\n",
        "  !mkdir -p data\n",
        "  !gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 shd_final data\n",
        "\n",
        "def download_image(path):\n",
        "  image = imageio.imread(path)\n",
        "  _image = image.astype('float32')\n",
        "  if GRAYSCALE:\n",
        "      _image = rgb2gray(_image / 255)\n",
        "  else:\n",
        "      _image = _image / 255\n",
        "  _image = resize(_image, IMAGE_SIZE)\n",
        "  return _image\n",
        "\n",
        "# TODO(Noah): Reimplement the code that sets up SH_RHD.\n",
        "gcs_path = 'data' if IN_COLAB else os.path.join(\"..\", \"SH_RHD\")\n",
        "train_list = os.listdir(os.path.join(gcs_path, \"training/color\"))\n",
        "eval_list = os.listdir(os.path.join(gcs_path, \"evaluation/color\"))\n",
        "\n",
        "# Below we implement stochastic subsampling of the train and eval list so\n",
        "# that our model will train in a reasonable amount of time.\n",
        "random.shuffle(train_list)\n",
        "random.shuffle(eval_list)\n",
        "\n",
        "# Setup some params.\n",
        "IMAGE_SIZE = 224\n",
        "GRAYSCALE = False\n",
        "IMAGE_CHANNELS = 1 if GRAYSCALE else 3\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# numpy \"buckets\" that we will use to load things in.\n",
        "x_train = np.zeros( (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS) )\n",
        "x_test = np.zeros( (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS) ) \n",
        "\n",
        "DESIRED_BATCH_COUNT = min(16, len(train_list) // BATCH_SIZE)\n",
        "print(cstr(\"DESIRED_BATCH_COUNT =\"), DESIRED_BATCH_COUNT)\n",
        "print(cstr(\"DESIRED_BATCH_COUNT\"), \"is the batches per epoch to train w/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMJu2pBWKJx8"
      },
      "outputs": [],
      "source": [
        "anno_train_path = os.path.join(\"data\", \"anno\", \"anno_training.pickle\") if IN_COLAB else \\\n",
        "    os.path.join(\"..\", \"RHD_small\", \"training\", \"anno_training.pickle\")\n",
        "anno_eval_path = os.path.join(\"data\", \"anno\", \"anno_evaluation.pickle\") if IN_COLAB else \\\n",
        "    os.path.join(\"..\", \"RHD_small\", \"evaluation\", \"anno_evaluation.pickle\")\n",
        "\n",
        "# NOTE: We note that the numbers 41258 and 2728 were retrieved directly from\n",
        "# https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html\n",
        "TRAIN_TOTAL_COUNT = 41258\n",
        "EVALUATION_TOTAL_COUNT = 2728\n",
        "\n",
        "y_train = np.zeros( (TRAIN_TOTAL_COUNT, 21, 3) )\n",
        "y_test = np.zeros( (EVALUATION_TOTAL_COUNT, 21, 3) )\n",
        "right_hands_train = []\n",
        "right_hands_test = []\n",
        "\n",
        "def load_anno(path, y, total_count, rh):\n",
        "  anno_all = []\n",
        "  count = 0\n",
        "  with open(path, 'rb') as f:\n",
        "    anno_all = pickle.load(f)\n",
        "  for key, value in anno_all.items():\n",
        "    if(count >= total_count):\n",
        "      break\n",
        "    kp_visible = (value['uv_vis'][:, 2] == 1)\n",
        "    case1 = np.sum(kp_visible[0:21])\n",
        "    case2 = np.sum(kp_visible[21:])\n",
        "    leftHand = case1 > 0\n",
        "    # NOTE: We note here that we are not checking if this training or evaluation example is valid.\n",
        "    # i.e. we want to densely store the annotations.\n",
        "    if(not leftHand):\n",
        "        y[count, :, :] = value['xyz'][21:42]\n",
        "    else: \n",
        "        y[count, :, :] = value['xyz'][:21]\n",
        "    rh.append(not leftHand)\n",
        "    count += 1\n",
        "\n",
        "print(\"Loading in training annotations\")\n",
        "time_start = time.time()\n",
        "load_anno(anno_train_path, y_train, TRAIN_TOTAL_COUNT, right_hands_train)\n",
        "time_end = time.time()\n",
        "print(cstr(\"Training annotations loaded in {} s\".format(time_end - time_start)))\n",
        "print(\"Loading in evaluation annotations\")\n",
        "time_start = time.time()\n",
        "load_anno(anno_eval_path, y_test, EVALUATION_TOTAL_COUNT, right_hands_test)\n",
        "time_end = time.time()\n",
        "print(cstr(\"Evaluation annotations loaded in {} s\".format(time_end - time_start)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSW4BpvZPIsb"
      },
      "source": [
        "# MODEL LOADING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLws7Z0QPIsb"
      },
      "outputs": [],
      "source": [
        "# TODO(Noah): Get the MANO folders hosted in GCS so that this works again.\n",
        "#   We note that this cost was tested and is in full working order, so \n",
        "#   the only thing not working is the lack of existence of MANO_DIR. \n",
        "\n",
        "MANO_DIR = os.path.join(\"data\", \"mano_v1_2\") if IN_COLAB else os.path.join(\"..\", \"mano_v1_2\")\n",
        "\n",
        "from mobilehand import MAKE_MOBILE_HAND\n",
        "from mobilehand_lfuncs import LOSS_3D\n",
        "\n",
        "MOBILE_HAND = MAKE_MOBILE_HAND(IMAGE_SIZE, IMAGE_CHANNELS, BATCH_SIZE, MANO_DIR)\n",
        "\n",
        "# INTEGRATION TEST\n",
        "input_test = tf.random.uniform(shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS))\n",
        "input_test = tf.cast(input_test, tf.float32)\n",
        "output_test = MOBILE_HAND(input_test)\n",
        "print(cstr(\"output_test =\"), output_test)\n",
        "\n",
        "# The lower training loop assumes that the model is set as such.\n",
        "model = MOBILE_HAND\n",
        "\n",
        "# The lower training loop also assumes that we have the loss function set like so.\n",
        "loss_fn = lambda pred, gt : LOSS_3D(pred,gt) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3xZXThyyVta"
      },
      "source": [
        "# TRAINING LOOP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fZ1XxeG3f3DD"
      },
      "outputs": [],
      "source": [
        "# with reference to this post https://www.codeitbro.com/send-email-using-python/#step-1-8211connect-to-the-mail-server. \n",
        "# Seems pretty bad tbh but it's gonna do the job??\n",
        "import smtplib\n",
        "import imghdr\n",
        "from email.message import EmailMessage\n",
        "\n",
        "Sender_Email = \"acc.cnoah@gmail.com\"\n",
        "Reciever_Email = \"cnoah1705@gmail.com\"\n",
        "Password = \"htqkbbitakdonazr\"\n",
        "\n",
        "def send_email(image_file):\n",
        "    newMessage = EmailMessage()                         \n",
        "    newMessage['Subject'] = \"New Checkpoint\" \n",
        "    newMessage['From'] = Sender_Email                   \n",
        "    newMessage['To'] = Reciever_Email                   \n",
        "    newMessage.set_content('Image attached!') \n",
        "\n",
        "    with open(image_file, 'rb') as f:\n",
        "        image_data = f.read()\n",
        "        image_type = imghdr.what(f.name)\n",
        "        image_name = f.name\n",
        "\n",
        "    newMessage.add_attachment(image_data, maintype='image', subtype=image_type, filename=image_name)\n",
        "\n",
        "    with smtplib.SMTP_SSL('smtp.gmail.com', 465) as smtp:\n",
        "        smtp.login(Sender_Email, Password)              \n",
        "        smtp.send_message(newMessage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpOZpsk5rNev"
      },
      "outputs": [],
      "source": [
        "class StupidSimpleLossMetric():\n",
        "    def __init__(self):\n",
        "        self.losses = [] # empty python array \n",
        "    def __call__(self, loss):\n",
        "        self.losses.append(loss)\n",
        "    def result(self):\n",
        "        return sum(self.losses) / len(self.losses)\n",
        "    def reset_states(self):\n",
        "        self.losses = []\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam() # defaults should work just fine\n",
        "train_loss = StupidSimpleLossMetric()\n",
        "test_loss = StupidSimpleLossMetric()\n",
        "\n",
        "# Loss function unit test\n",
        "input = tf.zeros([1, 21,3])  # mock pred of all zeros\n",
        "label = np.expand_dims(y_train[0], axis=0)\n",
        "loss = loss_fn(input, label) \n",
        "print('Loss for pred of all zeros', loss.numpy())\n",
        "#loss2 = loss_fn(label, label)\n",
        "#print('Loss for perfect prediction', loss2.numpy())\n",
        "input2 = tf.ones([1, 21, 3])\n",
        "loss3 = loss_fn(input2, label)\n",
        "print('Loss for pred of all ones', loss3.numpy())\n",
        "\n",
        "@tf.function\n",
        "def train_step(input, gt):\n",
        "    with tf.GradientTape() as tape:\n",
        "        mesh, keypoints = model(input)\n",
        "        #loss = loss_func(predictions, segmentation_masks)\n",
        "        #loss = np.dot(tf.reshape(segmentation_masks, [102400], tf.reshape(predictions, [102400])\n",
        "        loss = loss_fn(keypoints, gt)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "    #train_accuracy(labels, predictions)\n",
        "  \n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  # training=False is only needed if there are layers with different\n",
        "  # behavior during training versus inference (e.g. Dropout).\n",
        "  mesh, keypoints = model(images, training=False)\n",
        "  return loss_fn(keypoints, labels)\n",
        "  #test_accuracy(labels, predictions)\n",
        "\n",
        "checkpoint_path = os.path.join(\"data\", \"checkpoints\") if IN_COLAB else os.path.join(\"..\", \"checkpoints/\")\n",
        "\n",
        "if not IN_COLAB:\n",
        "  import open3d as o3d\n",
        "  import open3d.visualization.rendering as rendering\n",
        "  from mano_layer import MANO_Model\n",
        "\n",
        "  # ckpt_index is an index for the current checkpoint that the model param is loaded\n",
        "  # with weights.  \n",
        "  def render_checkpoint_image(ckpt_index, model, eval_image, annot):\n",
        "\n",
        "      # Step 1 is to use the eval_image in a forward pass w/ the model to generate a chkpt_image.\n",
        "      T_posed, keypoints3D = model(np.repeat(np.expand_dims(eval_image, 0), 32, axis=0))\n",
        "      #img_filepath = os.path.join(checkpoint_path, \"image-eval-{:0d}.png\".format(ckpt_index))\n",
        "      #imageio.imwrite(img_filepath, eval_image) # right away, let's go ahead and save the eval_image :)     \n",
        "\n",
        "      render = rendering.OffscreenRenderer(1080, 1080)\n",
        "      # TODO(Noah): Reloading MANO here is sort of redundant. We should expose the manu params on the\n",
        "      # model or something like that.\n",
        "      mano_dir = os.path.join(\"..\", \"mano_v1_2\")\n",
        "      mpi_model = MANO_Model(mano_dir) \n",
        "\n",
        "      green = rendering.MaterialRecord()\n",
        "      green.base_color = [0.0, 0.5, 0.0, 1.0]\n",
        "      green.shader = \"defaultLit\"\n",
        "      red = rendering.MaterialRecord()\n",
        "      red.base_color = [0.5, 0.0, 0.0, 1.0]\n",
        "      red.shader = \"defaultLit\"  \n",
        "      yellow = rendering.MaterialRecord()\n",
        "      yellow.base_color = [1.0, 0.75, 0.0, 1.0]\n",
        "      yellow.shader = \"defaultLit\"  \n",
        "\n",
        "      # k_y_batched = np.repeat(np.expand_dims(annot, axis=0), 21, axis=0)\n",
        "      # build the lines\n",
        "      lines = [ ]\n",
        "      for i in range(1, 21):\n",
        "          lines.append( \n",
        "              [\n",
        "                  i, \n",
        "                  mpi_model.RHD_K.numpy()[i]\n",
        "              ]\n",
        "          )\n",
        "      colors = [[1, 0, 0] for i in range(len(lines))]\n",
        "      line_set = o3d.geometry.LineSet()\n",
        "      line_set.points = o3d.utility.Vector3dVector(annot)\n",
        "      line_set.lines = o3d.utility.Vector2iVector(lines)\n",
        "      line_set.colors = o3d.utility.Vector3dVector(colors)\n",
        "      render.scene.add_geometry(\"line_set_RHD\", line_set, red)\n",
        "      i = 0\n",
        "      for i in range(21):\n",
        "          keypoint = annot[i]\n",
        "          msphere = o3d.geometry.TriangleMesh.create_sphere(0.005)\n",
        "          # msphere.paint_uniform_color([0.75, 1 - (j+1) / 5, (j+1) / 5])\n",
        "          msphere.compute_vertex_normals()\n",
        "          msphere.translate(keypoint)\n",
        "          render.scene.add_geometry(\"sphere_RHD{}\".format(i), msphere, yellow)\n",
        "\n",
        "      # [bs, 16, 3]\n",
        "      keypoints3D_pylist = tf.unstack( keypoints3D, axis=1 )\n",
        "      i = 0\n",
        "      for keypoint in keypoints3D_pylist:\n",
        "          keypoint = keypoint[0, :]\n",
        "          msphere = o3d.geometry.TriangleMesh.create_sphere(0.005)\n",
        "          msphere.paint_uniform_color([0, 0.75, 0])\n",
        "          msphere.compute_vertex_normals()\n",
        "          msphere.translate(keypoint.numpy())   \n",
        "          render.scene.add_geometry(\"sphere{}\".format(i), msphere, green)\n",
        "          i += 1\n",
        "      # build the lines\n",
        "      lines = [ ]\n",
        "      for i in range(1, 21):\n",
        "          lines.append( \n",
        "              [\n",
        "                  i, \n",
        "                  mpi_model.RHD_K.numpy()[i]\n",
        "              ]\n",
        "          )\n",
        "      colors = [[1, 0, 0] for i in range(len(lines))]\n",
        "      line_set = o3d.geometry.LineSet()\n",
        "      line_set.points = o3d.utility.Vector3dVector(keypoints3D[0, :, :].numpy())\n",
        "      line_set.lines = o3d.utility.Vector2iVector(lines)\n",
        "      line_set.colors = o3d.utility.Vector3dVector(colors)\n",
        "      render.scene.add_geometry(\"line_set\", line_set, red)\n",
        "\n",
        "      # add the MANO mesh as well.\n",
        "      T_posed_scaled = T_posed\n",
        "      mesh = o3d.geometry.TriangleMesh()\n",
        "      mesh.triangles = o3d.utility.Vector3iVector(mpi_model.F.numpy())\n",
        "      mesh.vertices = o3d.utility.Vector3dVector(T_posed_scaled[0, :, :].numpy()) \n",
        "      mesh.compute_vertex_normals()\n",
        "      pcd = mesh.sample_points_uniformly(number_of_points=1000)\n",
        "      render.scene.add_geometry(\"pcd\", pcd, red)  \n",
        "      \n",
        "      #cyl = o3d.geometry.TriangleMesh.create_cylinder(.05, 3)\n",
        "      #cyl.compute_vertex_normals()\n",
        "      #cyl.translate([-2, 0, 1.5])\n",
        "      #render.scene.add_geometry(\"cyl\", cyl, green)\n",
        "        \n",
        "      center = [0,0, annot[0][2]] # select the root annotation location\n",
        "      z_dist = 0.5  \n",
        "\n",
        "      # if we want to understand the params of the setup_camera func, it goes like this.\n",
        "      # (FOV, center, eye, up)\n",
        "      # when we setup the camera like we are doing here, it's the same as the openGL gl.lookAt()\n",
        "      # So for the explanation of the center, eye, and up params, see here \n",
        "      # https://stackoverflow.com/questions/21830340/understanding-glmlookat  \n",
        "      render.setup_camera(60.0, center, [center[0], center[1], center[2]-z_dist], [0, -1, 0])\n",
        "      render.scene.scene.set_sun_light([0.707, 0.0, -.707], [1.0, 1.0, 1.0],\n",
        "                                      75000)\n",
        "      render.scene.scene.enable_sun_light(True)\n",
        "      render.scene.show_axes(False)\n",
        "\n",
        "      img1 = render.render_to_image()\n",
        "      img_filepath = os.path.join(checkpoint_path, \"image-{:0d}.png\".format(ckpt_index))\n",
        "      print(cstr(\"Saving image(s) at\"), img_filepath)\n",
        "      #o3d.io.write_image(img_filepath, img, 9)\n",
        "\n",
        "      # TODO(Noah): Make it so that we render all of these images onto one big image in a grid\n",
        "      # for ease of viewing. AND, we can include the original image (the one passed into the model) \n",
        "      # into the grid.\n",
        "      # below, we are going to grab TWO more viewpoints of the same image\n",
        "      #img_filepath = os.path.join(checkpoint_path, \"image-left-{:0d}.png\".format(ckpt_index))\n",
        "      render.setup_camera(60.0, center, [center[0]-z_dist * 0.7071, center[1], center[2]-z_dist * 0.7071], [0, -1, 0])  \n",
        "      img2 = render.render_to_image()\n",
        "      #o3d.io.write_image(img_filepath, img, 9)\n",
        "      #img_filepath = os.path.join(checkpoint_path, \"image-right-{:0d}.png\".format(ckpt_index))\n",
        "      render.setup_camera(60.0, center, [center[0]+z_dist * 0.7071, center[1], center[2]-z_dist * 0.7071], [0, -1, 0])  \n",
        "      img3 = render.render_to_image()  \n",
        "      #o3d.io.write_image(img_filepath, img, 9)\n",
        "      \n",
        "      w = 10\n",
        "      h = 10\n",
        "      fig = plt.figure(figsize=(15, 10))\n",
        "      columns = 2\n",
        "      rows = 1\n",
        "      fig.add_subplot(rows, columns, 1)\n",
        "      plt.imshow(eval_image)\n",
        "      #fig.add_subplot(rows, columns, 2)\n",
        "      #plt.imshow(img2)\n",
        "      fig.add_subplot(rows, columns, 2)\n",
        "      plt.imshow(img1)\n",
        "      #fig.add_subplot(rows, columns, 4)\n",
        "      #plt.imshow(img3)\n",
        "\n",
        "      plt.savefig(img_filepath)\n",
        "\n",
        "\n",
        "      #send_email(img_filepath)\n",
        "      #print(cstr(\"Sent email of\"), img_filepath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYZawQmCPIsf"
      },
      "outputs": [],
      "source": [
        "!mkdir $checkpoint_path\n",
        "\n",
        "last_checkpoint = -1\n",
        "if (last_checkpoint > -1):\n",
        "  file_path = os.path.join(checkpoint_path, \"cp-{:04d}.ckpt\".format(last_checkpoint))\n",
        "  model.load_weights(file_path)\n",
        "  print(cstr(\"Loaded weights from {}\".format(file_path)))\n",
        "\n",
        "EPOCHS = 10 # sure...\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  print(\"Begin epoch\", epoch)\n",
        "  start = time.time()\n",
        "  train_loss.reset_states()\n",
        "  #train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  #test_accuracy.reset_states()\n",
        "  \n",
        "  y = np.zeros([BATCH_SIZE, 21, 3], dtype=np.float32)\n",
        "  \n",
        "  canonical_index = 0\n",
        "  for i in range(1):\n",
        "    count = 0\n",
        "    while count < BATCH_SIZE:\n",
        "      filename = train_list[canonical_index]\n",
        "      y_index = int(filename[0:5])\n",
        "      isRightHand = right_hands_train[y_index]\n",
        "      if isRightHand:\n",
        "        train_image = download_image(os.path.join(gcs_path, \"training\", \"color\", filename))\n",
        "        x_train[count,:,:,:] = train_image\n",
        "        y[count, :, :] = y_train[y_index]\n",
        "        count += 1\n",
        "      canonical_index += 1\n",
        "    x_train = x_train.astype('float32')\n",
        "    loss = train_step(x_train, y)\n",
        "    train_loss(loss.numpy())\n",
        "\n",
        "  canonical_index = 0\n",
        "  for i in range(1):\n",
        "    count = 0\n",
        "    while count < BATCH_SIZE:\n",
        "      filename = eval_list[canonical_index]\n",
        "      y_index = int(filename[0:5])\n",
        "      isRightHand = right_hands_test[y_index]\n",
        "      if isRightHand:\n",
        "        eval_image = download_image(os.path.join(gcs_path, \"evaluation\", \"color\", filename))\n",
        "        x_test[count,:,:,:] = eval_image\n",
        "        y[count, :, :] = y_test[y_index]\n",
        "        count += 1\n",
        "      canonical_index += 1\n",
        "    x_test = x_test.astype('float32')\n",
        "    loss_test = test_step(x_test, y)\n",
        "    test_loss(loss.numpy())\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  print(\n",
        "    f'Epoch {epoch}, '\n",
        "    f'Time {end-start} s'\n",
        "    f'Loss: {train_loss.result()}, '\n",
        "    f'Test Loss: {test_loss.result()}, '\n",
        "  )\n",
        "\n",
        "  # Save the model parameters\n",
        "  if (epoch % 5 == 0) or (epoch == EPOCHS - 1):\n",
        "    checkpoint_filepath = os.path.join(checkpoint_path, \"cp-{:04d}.ckpt\".format(epoch))\n",
        "    model.save_weights(checkpoint_filepath)\n",
        "    print(cstr(\"Saved weights to {}\".format(checkpoint_filepath)))\n",
        "\n",
        "    if not IN_COLAB:\n",
        "      # Run the model on image 19 of the evaluation images.\n",
        "      test_img = 19\n",
        "      eval_image = download_image(os.path.join(gcs_path, \"evaluation\", \"color\", \"000{}.png\".format(test_img)))\n",
        "      eval_image = eval_image.astype('float32')\n",
        "      render_checkpoint_image(epoch, model, eval_image, y_test[test_img])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test \"render_checkpoint_image\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the model on image 19 of the evaluation images.\n",
        "eval_image = download_image(os.path.join(gcs_path, \"evaluation\", \"color\", \"00019.png\"))\n",
        "eval_image = eval_image.astype('float32')\n",
        "annot = y_test[19]\n",
        "render_checkpoint_image(0, model, eval_image, annot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "rhd_eval_dir = os.path.join(gcs_path, \"evaluation\", \"color\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from evaluation import time_model\n",
        "time_model(model, rhd_eval_dir, download_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from evaluation import evaluate_model\n",
        "evaluate_model(model, rhd_eval_dir, download_image, y_test, right_hands_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FGNffUKbxtVJ"
      ],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "HandTracking.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
