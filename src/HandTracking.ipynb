{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BluBloos/3D-Hand-Tracking/blob/idea2/src/HandTracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytgQcZUUwswL"
      },
      "source": [
        "# SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VrrE2xwGZO2"
      },
      "source": [
        "IMAGE_SIZE, GRAYSCALE, and BATCH_SIZE should generally not be changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5jNtTsL-GZO3"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 224\n",
        "GRAYSCALE = False\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee72KKdTPIsZ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "########################################### HANDLE DIFFS WHEN RUNNING IN COLAB ##################################\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "print(\"In Colab:\", IN_COLAB)\n",
        "import sys\n",
        "\n",
        "if (IN_COLAB):\n",
        "  sys.path.insert(1, '/content/src/')\n",
        "  !echo \"Initializing github repository\"\n",
        "  !ls -la\n",
        "  !rm -r .config/\n",
        "  !rm -r sample_data/\n",
        "  !git clone https://github.com/BluBloos/QMIND2021-2022/ .  \n",
        "\n",
        "# Download updated project from Github.\n",
        "!git pull \n",
        "########################################### HANDLE DIFFS WHEN RUNNING IN COLAB ##################################\n",
        "\n",
        "########################################### EXTERNAL LIBRARIES ###########################################\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "#NOTE: Good resource. -> https://www.tensorflow.org/tutorials/quickstart/advanced\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, UpSampling2D, MaxPool2D\n",
        "from tensorflow.keras import Model\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "import random\n",
        "from qmindcolors import cstr\n",
        "import cv2\n",
        "########################################### EXTERNAL LIBRARIES ###########################################\n",
        "\n",
        "########################################### DATA LOADING ###########################################\n",
        "IMAGE_CHANNELS = 1 if GRAYSCALE else 3\n",
        "\n",
        "# NOTE(Noah): gcs code will only work on the Colab. It works on either Ubuntu or macOS (no Windows support).\n",
        "# I attempted to install gcsfuse on my macOS machine, but it did not work.\n",
        "# gsfuse is beta software.\n",
        "if IN_COLAB:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  # we know that we are on an Ubuntu machine.\n",
        "  # Thus, installing gcsfuse will be done via the Ubuntu instructions.\n",
        "  # https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/installing.md#ubuntu-and-debian-latest-releases\n",
        "  !echo \"deb http://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" \\\n",
        "    | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "  !curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "  # -y in apt-get will assume \"yes\" as the answer to all prompts.\n",
        "  # -q in apt-get will make things \"quiet\" for us. Nice!\n",
        "  !sudo apt-get -y -q update\n",
        "  !sudo apt-get -y -q install gcsfuse\n",
        "  !mkdir -p data\n",
        "  !gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 shd_final data\n",
        "\n",
        "# NOTE(Noah): Stole this function from Stackoverflow :)\n",
        "def rgb2gray(rgb):\n",
        "  return np.expand_dims(np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140]), axis=2)\n",
        "    \n",
        "def resize(img, size):\n",
        "  return cv2.resize(img, dsize=(size, size), interpolation=cv2.INTER_CUBIC)\n",
        "  \n",
        "def download_image(root_dir, set, img_index):\n",
        "  file_path = os.path.join(root_dir, set, \"color\", \"{:05d}.png\".format(img_index))\n",
        "  image = imageio.imread(file_path)\n",
        "  _image = image.astype('float32')\n",
        "  if GRAYSCALE:\n",
        "    _image = rgb2gray(_image)\n",
        "  else:\n",
        "    _image = _image\n",
        "\n",
        "  annot_2D = y2_train[img_index]\n",
        "  if set == \"evaluation\":\n",
        "    annot_2D = y2_test[img_index]\n",
        "  pixel_trans = np.array([160,160]) - annot_2D[0]\n",
        "  x_shift = int(pixel_trans[0])\n",
        "  y_shift = int(pixel_trans[1])\n",
        "  _image = np.roll( _image, (y_shift, x_shift), axis=(0,1) )\n",
        "\n",
        "  # black out the regions we do not care about\n",
        "  if y_shift > 0:\n",
        "    _image = cv2.rectangle(_image, (0, 0), (320, y_shift), 0, -1)\n",
        "  else:\n",
        "    _image = cv2.rectangle(_image, (0, 320 + y_shift), (320, 320), 0, -1)\n",
        "\n",
        "  if x_shift > 0:\n",
        "    _image = cv2.rectangle(_image, (0, 0), (x_shift, 320), 0, -1)\n",
        "  else:\n",
        "    _image = cv2.rectangle(_image, (320 + x_shift, 0), (320, 320), 0, -1)\n",
        "\n",
        "  _image = resize(_image, IMAGE_SIZE)\n",
        "\n",
        "  return _image\n",
        "\n",
        "# TODO(Noah): Reimplement the code that sets up SH_RHD.\n",
        "gcs_path = 'data' if IN_COLAB else os.path.join(\"..\", \"SH_RHD\")\n",
        "train_list = os.listdir(os.path.join(gcs_path, \"training/color\"))\n",
        "eval_list = os.listdir(os.path.join(gcs_path, \"evaluation/color\"))\n",
        "\n",
        "train_list = sorted(train_list)\n",
        "eval_list = sorted(eval_list)\n",
        "\n",
        "# Below, we implement stochastic subsampling of the train and eval list so\n",
        "# that our model will train in a reasonable amount of time.\n",
        "DESIRED_BATCH_COUNT = min(60, len(train_list) // BATCH_SIZE)\n",
        "print(cstr(\"DESIRED_BATCH_COUNT =\"), DESIRED_BATCH_COUNT)\n",
        "print(cstr(\"DESIRED_BATCH_COUNT\"), \"is the batches per epoch to train w/\")\n",
        "DESIRED_TEST_BATCH_COUNT = min(1, len(eval_list) // BATCH_SIZE)\n",
        "print(cstr(\"DESIRED_TEST_BATCH_COUNT =\"), DESIRED_BATCH_COUNT)\n",
        "print(cstr(\"DESIRED_TEST_BATCH_COUNT\"), \"is the batches per epoch to test w/\")\n",
        "########################################### DATA LOADING ###########################################\n",
        "\n",
        "########################################### LOAD ANNOTATIONS ###########################################\n",
        "anno_train_path = os.path.join(\"data\", \"anno\", \"anno_training.pickle\") if IN_COLAB else \\\n",
        "    os.path.join(\"..\", \"RHD_small\", \"training\", \"anno_training.pickle\")\n",
        "anno_eval_path = os.path.join(\"data\", \"anno\", \"anno_evaluation.pickle\") if IN_COLAB else \\\n",
        "    os.path.join(\"..\", \"RHD_small\", \"evaluation\", \"anno_evaluation.pickle\")\n",
        "\n",
        "# NOTE: We note that the numbers 41258 and 2728 were retrieved directly from\n",
        "# https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html\n",
        "TRAIN_TOTAL_COUNT = 41258\n",
        "EVALUATION_TOTAL_COUNT = 2728\n",
        "\n",
        "y_train = np.zeros((TRAIN_TOTAL_COUNT, 21, 3), dtype=np.float32)\n",
        "y_test = np.zeros((EVALUATION_TOTAL_COUNT, 21, 3), dtype=np.float32)\n",
        "k_train = np.zeros((TRAIN_TOTAL_COUNT, 3, 3), dtype=np.float32)\n",
        "k_test = np.zeros((EVALUATION_TOTAL_COUNT, 3, 3), dtype=np.float32)\n",
        "y2_train = np.zeros((TRAIN_TOTAL_COUNT, 21, 2), dtype=np.float32)\n",
        "y2_test = np.zeros((EVALUATION_TOTAL_COUNT, 21, 2), dtype=np.float32)\n",
        "\n",
        "def load_anno(path, y, k, y2):\n",
        "  anno_all = []\n",
        "  count = 0\n",
        "  with open(path, 'rb') as f:\n",
        "    anno_all = pickle.load(f)\n",
        "  for key, value in anno_all.items():\n",
        "    kp_visible = (value['uv_vis'][:, 2] == 1)\n",
        "    case1 = np.sum(kp_visible[0:21])\n",
        "    case2 = np.sum(kp_visible[21:])\n",
        "    leftHand = case1 > 0\n",
        "    # NOTE: We note here that we are not checking if this training or evaluation example is valid.\n",
        "    # i.e. we want to densely store the annotations.\n",
        "    if(not leftHand):\n",
        "        y[count, :, :] = np.array(value['xyz'][21:42], dtype=np.float32)\n",
        "        y2[count, :, :] = np.array(value['uv_vis'][:, :2][21:42], dtype=np.float32)\n",
        "    else: \n",
        "        y[count, :, :] = np.array(value['xyz'][:21], dtype=np.float32)\n",
        "        y2[count, :, :] = np.array(value['uv_vis'][:, :2][:21], dtype=np.float32)\n",
        "\n",
        "    # Adjust the 3D keypoints to be at the center of the image.\n",
        "    annot_3D = y[count, :, :]\n",
        "    y[count, :, :] -= np.array([annot_3D[0][0], annot_3D[0][1], 0.0], dtype=np.float32)\n",
        "\n",
        "    k[count, :, :] = value['K']\n",
        "    count += 1\n",
        "\n",
        "print(\"Loading in training annotations\")\n",
        "time_start = time.time()\n",
        "load_anno(anno_train_path, y_train, k_train, y2_train)\n",
        "time_end = time.time()\n",
        "print(cstr(\"Training annotations loaded in {} s\".format(time_end - time_start)))\n",
        "print(\"Loading in evaluation annotations\")\n",
        "time_start = time.time()\n",
        "load_anno(anno_eval_path, y_test, k_test, y2_test)\n",
        "time_end = time.time()\n",
        "print(cstr(\"Evaluation annotations loaded in {} s\".format(time_end - time_start)))\n",
        "########################################### LOAD ANNOTATIONS ###########################################\n",
        "\n",
        "########################################### MODEL LOADING ###########################################\n",
        "tf.keras.backend.clear_session()\n",
        "MANO_DIR = os.path.join(\"data\", \"mano_v1_2\") if IN_COLAB else os.path.join(\"..\", \"mano_v1_2\")\n",
        "from mobilehand import MAKE_MOBILE_HAND\n",
        "from mobilehand_lfuncs import LOSS_3D\n",
        "model = MAKE_MOBILE_HAND(IMAGE_SIZE, IMAGE_CHANNELS, BATCH_SIZE, MANO_DIR)\n",
        "\n",
        "### MODEL FORWARD PASS TEST ###\n",
        "input_test = tf.random.uniform(shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS))\n",
        "input_test = tf.cast(input_test, tf.float32)\n",
        "output_test = model(input_test) \n",
        "print(cstr(\"output_test =\"), output_test)\n",
        "### MODEL FORWARD PASS TEST ###\n",
        "\n",
        "from mobilehand_lfuncs import LOSS\n",
        "from mano_layer import MANO_Model\n",
        "_mpi_model = MANO_Model(MANO_DIR)\n",
        "# TODO(Noah): Expose U and L directly on our mobilehand implementation.\n",
        "U = _mpi_model.U\n",
        "L = _mpi_model.L\n",
        "loss_fn = lambda beta, pose, L, U, cam_R, depth, scale, pred, gt : \\\n",
        "    LOSS(beta, pose, L, U, cam_R, depth, scale, pred, gt)\n",
        "########################################### MODEL LOADING ###########################################\n",
        "\n",
        "########################################### TRAINING SETUP ###########################################\n",
        "class StupidSimpleLossMetric():\n",
        "    def __init__(self):\n",
        "        self.losses = [] # empty python array \n",
        "    def __call__(self, loss):\n",
        "        self.losses.append(loss)\n",
        "    def result(self):\n",
        "        return sum(self.losses) / len(self.losses)\n",
        "    def reset_states(self):\n",
        "        self.losses = []\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam() # defaults should work just fine\n",
        "train_loss = StupidSimpleLossMetric()\n",
        "test_loss = StupidSimpleLossMetric()\n",
        "\n",
        "@tf.function\n",
        "def train_step(input, scale, z_depth, gt):\n",
        "    with tf.GradientTape() as tape:\n",
        "        beta, pose, mesh, keypoints, cam_R = model(input)\n",
        "        #loss = loss_func(predictions, segmentation_masks)\n",
        "        #loss = np.dot(tf.reshape(segmentation_masks, [102400], tf.reshape(predictions, [102400])\n",
        "        #loss = loss_fn(keypoints, gt)\n",
        "        loss = loss_fn(beta, pose, L, U, cam_R, z_depth, scale, keypoints, gt)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    #print(cstr(\"model.trainable_variables\"), model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "  \n",
        "@tf.function\n",
        "def test_step(images, scale, z_depth, labels):\n",
        "  # training=False is only needed if there are layers with different\n",
        "  # behavior during training versus inference (e.g. Dropout).\n",
        "  beta, pose, mesh, keypoints, cam_R = model(images, training=False)\n",
        "  return loss_fn(beta, pose, L, U, cam_R, z_depth, scale, keypoints, labels)\n",
        "  #test_accuracy(labels, predictions)\n",
        "\n",
        "checkpoint_path = os.path.join(\"data\", \"checkpoints\") if IN_COLAB else os.path.join(\"..\", \"checkpoints/\")\n",
        "\n",
        "if not IN_COLAB:\n",
        "    from render_ckpt import render_checkpoint_image\n",
        "########################################### TRAINING SETUP ###########################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnvzoocNGZO8"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "The variable LAST_CHECKPOINT controls where our model training will start off from.\n",
        "Leave as -1 to \"start fresh\".\n",
        "OR adjust to any number, so long as there is a checkpoint saved for that number.\n",
        "\n",
        "You may also adjust EPOCHS as you wish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Hcqltle4GZO9"
      },
      "outputs": [],
      "source": [
        "LAST_CHECKPOINT = -1\n",
        "EPOCHS = 10 # sure..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYZawQmCPIsf"
      },
      "outputs": [],
      "source": [
        "!mkdir $checkpoint_path\n",
        "\n",
        "random.shuffle(train_list)\n",
        "random.shuffle(eval_list)\n",
        "# numpy \"buckets\" that we will use to load things in.\n",
        "x_train = np.zeros( (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS) )\n",
        "x_test = np.zeros( (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS) ) \n",
        "\n",
        "LAST_CHECKPOINT = -1\n",
        "if (LAST_CHECKPOINT > -1):\n",
        "  file_path = os.path.join(checkpoint_path, \"cp-{:04d}.ckpt\".format(LAST_CHECKPOINT))\n",
        "  model.load_weights(file_path)\n",
        "  print(cstr(\"Loaded weights from {}\".format(file_path)))\n",
        "LAST_CHECKPOINT = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  print(\"Begin epoch\", epoch)\n",
        "  start = time.time()\n",
        "  train_loss.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  \n",
        "  y = np.zeros([BATCH_SIZE, 21, 3], dtype=np.float32)\n",
        "  \n",
        "  for i in range(DESIRED_BATCH_COUNT):\n",
        "    for j in range(BATCH_SIZE):\n",
        "      filename = train_list[j + i * BATCH_SIZE]\n",
        "      y_index = int(filename[0:5])\n",
        "      train_image = download_image(gcs_path, \"training\", y_index)\n",
        "      x_train[j,:,:,:] = train_image\n",
        "      y[j, :, :] = y_train[y_index]\n",
        "\n",
        "    x_train = x_train.astype('float32')\n",
        "    \n",
        "    # TODO(Noah): Do something with this constant value as it is used quite a bit thru the codebase.\n",
        "    scale = np.sqrt(np.sum(\n",
        "      np.square(y[:, 0] - y[:, 8]), axis=1, keepdims=True)) / 0.1537328322252615\n",
        "    z_depth = tf.constant(y[:, 0])\n",
        "\n",
        "    loss = train_step(x_train, scale, z_depth, y)\n",
        "    train_loss(loss.numpy())\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  print(\n",
        "    f'Epoch {epoch}, '\n",
        "    f'Time {end-start} s'\n",
        "    f'Loss: {train_loss.result()}, '\n",
        "    #f'Test Loss: {test_loss.result()}, '\n",
        "  )\n",
        "\n",
        "  # Save the model parameters\n",
        "  if (epoch % 5 == 0) or (epoch == EPOCHS - 1):\n",
        "    ckpt_index = LAST_CHECKPOINT + epoch\n",
        "    checkpoint_filepath = os.path.join(checkpoint_path, \"cp-{:04d}.ckpt\".format(ckpt_index))\n",
        "    model.save_weights(checkpoint_filepath)\n",
        "    print(cstr(\"Saved weights to {}\".format(checkpoint_filepath)))\n",
        "\n",
        "    if not IN_COLAB:\n",
        "      # Run the model on image 19 of the evaluation images.\n",
        "      test_img = 19\n",
        "      eval_image = download_image(gcs_path, \"evaluation\", test_img)\n",
        "      eval_image = eval_image.astype('float32')\n",
        "      annot = (y2_test[test_img], y_test[test_img], k_test[test_img])\n",
        "      render_checkpoint_image(checkpoint_path, ckpt_index, model, eval_image, annot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGoPFFL_GZO_"
      },
      "source": [
        "## Training Loop to Test Overfitting\n",
        "\n",
        "Here we implement a training loop to test if we can overfit the model on just one image. We believe that this test will generally validate our model architecture and it's trainability. We have been having major issues with underfitting, and we belive that passing this overfit test is key to our success."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJjVSBcHGZPA"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG5aWpBmGZPA"
      },
      "outputs": [],
      "source": [
        "!mkdir $checkpoint_path\n",
        "\n",
        "last_checkpoint = -1\n",
        "if (last_checkpoint > -1):\n",
        "  file_path = os.path.join(checkpoint_path, \"cp-{:04d}.ckpt\".format(last_checkpoint))\n",
        "  model.load_weights(file_path)\n",
        "  print(cstr(\"Loaded weights from {}\".format(file_path)))\n",
        "\n",
        "# load the crap AND we only have to do it ONE time because we are simply trying to overfit\n",
        "# on the model.\n",
        "y = np.zeros([BATCH_SIZE, 21, 3], dtype=np.float32)\n",
        "x_train = np.zeros((BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS))\n",
        "selected_image = -1 # selected image\n",
        "for j in range(BATCH_SIZE):\n",
        "  filename = train_list[3]\n",
        "  selected_image = int(filename[0:5])\n",
        "  train_image = download_image(gcs_path, \"training\", selected_image)\n",
        "  x_train[j,:,:,:] = train_image\n",
        "  y[j, :, :] = y_train[selected_image]\n",
        "\n",
        "print(cstr(\"selected_image\"), selected_image)\n",
        "print(x_train[0])\n",
        "\n",
        "# TODO(Noah): We are having serious issues with modularity of code. But we need to iterate fast and\n",
        "# get something done.\n",
        "scale = np.sqrt(np.sum(\n",
        "  np.square(y[:, 0] - y[:, 8]), axis=1, keepdims=True)) / 0.1537328322252615\n",
        "z_depth = tf.constant(y[:, 0])\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  print(\"Begin epoch\", epoch)\n",
        "  start = time.time()\n",
        "  train_loss.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  \n",
        "  x_train = x_train.astype('float32')\n",
        "  loss = train_step(x_train, scale, z_depth, y)\n",
        "  train_loss(loss.numpy())\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  print(\n",
        "    f'Epoch {epoch}, '\n",
        "    f'Time {end-start} s'\n",
        "    f'Loss: {train_loss.result()}, '\n",
        "  )\n",
        "\n",
        "  # Save the model parameters for EACH EPOCH\n",
        "  checkpoint_filepath = os.path.join(checkpoint_path, \"cp-{:04d}.ckpt\".format(epoch))\n",
        "  model.save_weights(checkpoint_filepath)\n",
        "  print(cstr(\"Saved weights to {}\".format(checkpoint_filepath)))\n",
        "\n",
        "  if not IN_COLAB:\n",
        "    # Run the model on image 19 of the evaluation images.\n",
        "    test_img = selected_image\n",
        "    eval_image = download_image(gcs_path, \"training\", test_img)\n",
        "    eval_image = eval_image.astype('float32')\n",
        "    annot = (y2_train[test_img], y_train[test_img], k_train[test_img])\n",
        "    render_checkpoint_image(checkpoint_path, epoch, model, eval_image, annot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWmzNQL_GZPC"
      },
      "source": [
        "## Test \"render_checkpoint_image\"\n",
        "\n",
        "The block below will test the render_checkpoint_image subroutine. We load image 19 from the evaluation set of RHD as input into the subroutine. \n",
        "\n",
        "Note the template_override parameter of the function call. Setting this to true will ignore the evalution image and no forward pass will happen. Instead, the MANO template mesh will be rendered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VRBxJUBGZPC"
      },
      "outputs": [],
      "source": [
        "IMG = 26\n",
        "# Run the model on image 19 of the evaluation images.\n",
        "eval_image = download_image(gcs_path, \"training\", IMG)\n",
        "eval_image = eval_image.astype('float32')\n",
        "\n",
        "annot_3D = y_train[IMG]\n",
        "annot_2D = y2_train[IMG]\n",
        "annot_K = k_train[IMG]\n",
        "annot = (annot_2D, annot_3D, annot_K)\n",
        "\n",
        "render_checkpoint_image(checkpoint_path, 0, model, eval_image, annot, camR_override=tf.constant([[0.0, 1.5707963, 0.0]]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UehPo5GNGZPD"
      },
      "source": [
        "## Test loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZe31ezeGZPD"
      },
      "outputs": [],
      "source": [
        "IMG = 26\n",
        "# Run the model on image 19 of the evaluation images.\n",
        "eval_image = download_image(gcs_path, \"training\", IMG)\n",
        "eval_image = eval_image.astype('float32')\n",
        "\n",
        "annot_3D = y_train[IMG].astype('float32')\n",
        "annot_2D = y2_train[IMG]\n",
        "annot_K = k_train[IMG]\n",
        "annot = (annot_2D, annot_3D, annot_K)\n",
        "\n",
        "camR_override=tf.constant([[0, 0, 0.5235988]])\n",
        "#camR_override=tf.constant([[0, 0.0, 0.0]])\n",
        "scale = np.sqrt(np.sum(np.square(annot_3D[0] - annot_3D[8]), axis=0, keepdims=True)) / 0.1537328322252615 \n",
        "scale = np.repeat(np.expand_dims(scale, axis=0), repeats=32, axis=0)\n",
        "z_depth = tf.repeat(tf.constant([[0, 0, annot_3D[0][2]]]), repeats=32, axis=0)\n",
        "# Step 1 is to use the eval_image in a forward pass w/ the model to generate a ckpt_image.\n",
        "_beta, _pose, T_posed, keypoints3D, cam_R = model(\n",
        "    np.repeat(np.expand_dims(eval_image, 0), 32, axis=0))\n",
        "cam_R = tf.repeat(camR_override, repeats=32, axis=0)\n",
        "\n",
        "from mobilehand_lfuncs import LOSS_2D\n",
        "from mobilehand_lfuncs import LOSS_3D\n",
        "from mobilehand_lfuncs import LOSS_REG\n",
        "\n",
        "loss2D = LOSS_2D(cam_R, z_depth, scale, keypoints3D, annot_3D) #loss_fn(_beta, _pose, L, U, cam_R, z_depth, scale, keypoints3D, annot_3D)\n",
        "print(cstr(\"loss2D\"), loss2D)\n",
        "loss3D = LOSS_3D(cam_R, z_depth, scale, keypoints3D, annot_3D)\n",
        "print(cstr(\"loss3D\"), loss3D)\n",
        "loss_reg = LOSS_REG(_beta, _pose, L, U)\n",
        "print(cstr(\"loss_reg\"), loss_reg)\n",
        "\n",
        "render_checkpoint_image(checkpoint_path, 0, model, eval_image, annot, camR_override=camR_override)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy5kUeERGZPD"
      },
      "source": [
        "# MODEL EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HawwGaRbGZPE"
      },
      "outputs": [],
      "source": [
        "rhd_eval_dir = os.path.join(gcs_path, \"evaluation\", \"color\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-5LwtZVGZPE"
      },
      "outputs": [],
      "source": [
        "from evaluation import time_model\n",
        "time_model(model, rhd_eval_dir, download_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGjqilwGGZPE"
      },
      "outputs": [],
      "source": [
        "from evaluation import evaluate_model\n",
        "evaluate_model(model, rhd_eval_dir, download_image, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvfcO_kKGZPF"
      },
      "outputs": [],
      "source": [
        "from evaluation import generate_loss_graph\n",
        "generate_loss_graph(checkpoint_path, model, \n",
        "    rhd_eval_dir, download_image, y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FGNffUKbxtVJ"
      ],
      "machine_shape": "hm",
      "name": "HandTracking.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}