{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/BluBloos/3D-Hand-Tracking/blob/idea2/src/HandTracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytgQcZUUwswL"
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee72KKdTPIsZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################### PARAMS ########################################### \n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 224\n",
    "GRAYSCALE = False\n",
    "IMAGE_CHANNELS = 1 if GRAYSCALE else 3\n",
    "\n",
    "########################################### HANDLE DIFFS WHEN RUNNING IN COLAB ##################################\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "import sys\n",
    "if (IN_COLAB):\n",
    "  sys.path.insert(1, '/content/src/')\n",
    "  !echo \"Initializing github repository\"\n",
    "  !ls -la\n",
    "  !rm -r .config/\n",
    "  !rm -r sample_data/\n",
    "  !git clone https://github.com/BluBloos/QMIND2021-2022/ .  \n",
    "\n",
    "!git pull \n",
    "\n",
    "########################################### EXTERNAL LIBRARIES ###########################################\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "import random\n",
    "from importlib import reload\n",
    "\n",
    "########################################### INIT QMIND_LIB ###########################################\n",
    "\n",
    "# NOTE(Noah): gcs code will only work on the Colab. It works on either Ubuntu or macOS (no Windows support).\n",
    "# I attempted to install gcsfuse on my macOS machine, but it did not work.\n",
    "# gsfuse is beta software.\n",
    "if IN_COLAB:\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "  # we know that we are on an Ubuntu machine.\n",
    "  # Thus, installing gcsfuse will be done via the Ubuntu instructions.\n",
    "  # https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/installing.md#ubuntu-and-debian-latest-releases\n",
    "  !echo \"deb http://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" \\\n",
    "    | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
    "  !curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "  # -y in apt-get will assume \"yes\" as the answer to all prompts.\n",
    "  # -q in apt-get will make things \"quiet\" for us. Nice!\n",
    "  !sudo apt-get -y -q update\n",
    "  !sudo apt-get -y -q install gcsfuse\n",
    "  !mkdir -p data\n",
    "  !gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 shd_final data\n",
    "\n",
    "import qmind_lib as qmindlib\n",
    "reload(qmindlib)\n",
    "\n",
    "rhd_dir = 'data' if IN_COLAB else os.path.join(\"..\", \"SH_RHD\")\n",
    "qmindlib.init(rhd_dir)\n",
    "\n",
    "cstr = qmindlib.cstr\n",
    "y_train = qmindlib.y_train\n",
    "y_test = qmindlib.y_test\n",
    "k_train = qmindlib.k_train\n",
    "k_test = qmindlib.k_test\n",
    "y2_train = qmindlib.y2_train\n",
    "y2_test = qmindlib.y2_test\n",
    "\n",
    "########################################### MODEL LOADING ###########################################\n",
    "tf.keras.backend.clear_session()\n",
    "MANO_DIR = os.path.join(\"data\", \"mano_v1_2\") if IN_COLAB else os.path.join(\"..\", \"mano_v1_2\")\n",
    "import mobilehand\n",
    "reload(mobilehand)\n",
    "model = mobilehand.MobileHand(IMAGE_SIZE, IMAGE_CHANNELS, MANO_DIR)\n",
    "model.mobile_net.unfreeze()\n",
    "\n",
    "### MODEL FORWARD PASS TEST ###\n",
    "input_test = tf.random.uniform(shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS), dtype=tf.float32)\n",
    "output_test = model(input_test) \n",
    "print(cstr(\"output_test =\"), output_test)\n",
    "### MODEL FORWARD PASS TEST ###\n",
    "\n",
    "U = model.mano_model.U\n",
    "L = model.mano_model.L\n",
    "loss_fn = mobilehand.LOSS\n",
    "\n",
    "########################################### TRAINING SETUP ###########################################\n",
    "class StupidSimpleLossMetric():\n",
    "    def __init__(self):\n",
    "        self.losses = [] # empty python array \n",
    "    def __call__(self, loss):\n",
    "        self.losses.append(loss)\n",
    "    def result(self):\n",
    "        return sum(self.losses) / len(self.losses)\n",
    "    def reset_states(self):\n",
    "        self.losses = []\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() # defaults should work just fine\n",
    "train_loss = StupidSimpleLossMetric()\n",
    "\n",
    "@tf.function\n",
    "def train_step(input, gt):\n",
    "  with tf.GradientTape() as tape:\n",
    "    beta, pose, mesh, keypoints, scale = model(input)\n",
    "    \n",
    "    # This is the thing that takes our MANO template to the same shape as gt.\n",
    "    gt_scale = tf.sqrt(tf.reduce_sum(tf.square(gt[:, 0] - gt[:, 8]), axis=1, keepdims=True)) / 0.0906426\n",
    "    gt_scale = tf.expand_dims(gt_scale, axis=1) # should have shape = [bs, 1, 1]\n",
    "    \n",
    "    loss = loss_fn(beta, pose, L, U, scale, keypoints, gt, gt_scale)\n",
    "\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  return loss\n",
    "\n",
    "if not IN_COLAB:\n",
    "    import render_ckpt\n",
    "    reload(render_ckpt)\n",
    "    render_checkpoint_image = render_ckpt.render_checkpoint_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnvzoocNGZO8"
   },
   "source": [
    "# TRAINING\n",
    "The variable LAST_CHECKPOINT controls where our model training will start off from.\n",
    "Leave as -1 to \"start fresh\".\n",
    "OR adjust to any number, so long as there is a checkpoint saved for that number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYZawQmCPIsf",
    "outputId": "6fce7f60-b1f4-4b6c-e289-e509aa94add2"
   },
   "outputs": [],
   "source": [
    "########################################### PARAMS ###########################################\n",
    "BATCH_SIZE = 47\n",
    "LAST_CHECKPOINT = 2333\n",
    "EPOCHS = 500 # sure...\n",
    "optimizer.learning_rate = 1e-5\n",
    "\n",
    "train_list = qmindlib.get_train_list()\n",
    "random.shuffle(train_list)\n",
    "\n",
    "DESIRED_BATCH_COUNT = min(60, len(train_list) // BATCH_SIZE)\n",
    "print(cstr(\"DESIRED_BATCH_COUNT =\"), DESIRED_BATCH_COUNT)\n",
    "print(cstr(\"DESIRED_BATCH_COUNT\"), \"is the batches per epoch to train w/\")\n",
    "\n",
    "########################################### SETUP ###########################################\n",
    "checkpoint_path = os.path.join(\"data\", \"checkpoints\") if IN_COLAB else os.path.join(\"..\", \"checkpoints/\")\n",
    "!mkdir $checkpoint_path\n",
    "\n",
    "# numpy \"bucket\" that we will use to load things in.\n",
    "x_train = np.zeros( (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS) )\n",
    "y = np.zeros([BATCH_SIZE, 21, 3], dtype=np.float32)\n",
    "\n",
    "if (LAST_CHECKPOINT > -1):\n",
    "  file_path = os.path.join(checkpoint_path, \"cp-{:04d}.ckpt\".format(LAST_CHECKPOINT))\n",
    "  model.load_weights(file_path)\n",
    "  print(cstr(\"Loaded weights from {}\".format(file_path)))\n",
    "else:\n",
    "  LAST_CHECKPOINT = 0\n",
    "\n",
    "########################################### TRAINING LOOP ###########################################\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "  print(\"Begin epoch\", epoch)\n",
    "  train_loss.reset_states()\n",
    "  start = time.time()\n",
    "  \n",
    "  for i in range(DESIRED_BATCH_COUNT):\n",
    "    for j in range(BATCH_SIZE):\n",
    "      filename = train_list[j + i * BATCH_SIZE]\n",
    "      y_index = int(filename[0:5])\n",
    "      train_image = qmindlib.download_image(\"training\", y_index)\n",
    "      x_train[j,:,:,:] = train_image\n",
    "      y[j, :, :] = y_train[y_index]\n",
    "    loss = train_step(x_train, y)\n",
    "    train_loss(loss.numpy())\n",
    "\n",
    "  end = time.time()\n",
    "\n",
    "  print(\n",
    "    f'Epoch {epoch}, '\n",
    "    f'Time {end-start} s'\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "  )\n",
    "\n",
    "  # Save the model parameters\n",
    "  ckpt_index = LAST_CHECKPOINT + epoch\n",
    "  checkpoint_filepath = os.path.join(checkpoint_path, \"cp-{:04d}.ckpt\".format(ckpt_index))\n",
    "  model.save_weights(checkpoint_filepath)\n",
    "  print(cstr(\"Saved weights to {}\".format(checkpoint_filepath)))\n",
    "\n",
    "  if not IN_COLAB:\n",
    "    # Run the model on image 19 of the evaluation images.\n",
    "    test_img = 26\n",
    "    eval_image = qmindlib.download_image(\"training\", test_img)\n",
    "    annot = (y2_train[test_img], y_train[test_img], k_train[test_img])\n",
    "    render_checkpoint_image(checkpoint_path, ckpt_index, model, eval_image, annot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cy5kUeERGZPD"
   },
   "source": [
    "# MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGjqilwGGZPE"
   },
   "outputs": [],
   "source": [
    "import evaluation\n",
    "reload(evaluation)\n",
    "\n",
    "# load desired model weights\n",
    "checkpoint_path = os.path.join('../','checkpoints', \"cp-{:04d}.ckpt\".format(2333))\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "train_list = qmindlib.get_train_list()\n",
    "evaluation.evaluate_model(model, train_list, \"training\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "FGNffUKbxtVJ"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "HandTracking.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "92e3dde21320b21078bf5b8c27013c99b8378d97a0c50fdf82b9dbd22849a6e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('3.9.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
