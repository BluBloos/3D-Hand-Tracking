{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BluBloos/3D-Hand-Tracking/blob/main/src/HandTracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytgQcZUUwswL"
      },
      "source": [
        "# SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFT4_Ezkg0cx"
      },
      "source": [
        "## Run ONLY when in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_ZBgm__TNlq"
      },
      "outputs": [],
      "source": [
        "!echo \"Initializing github repository\"\n",
        "!ls -la\n",
        "!rm -r .config/\n",
        "!rm -r sample_data/\n",
        "!git clone https://github.com/BluBloos/QMIND2021-2022/ ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7uiZv5Lg0cz"
      },
      "source": [
        "## Always Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ee72KKdTPIsZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: invalid refspec 'Github.'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In Colab: False\n",
            "TensorFlow version: 2.8.0\n",
            "Requirement already satisfied: chumpy in c:\\users\\maxim\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.70)\n",
            "Requirement already satisfied: six>=1.11.0 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chumpy) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.13.0 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from chumpy) (1.7.1)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\users\\maxim\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scipy>=0.13.0->chumpy) (1.22.3)\n"
          ]
        }
      ],
      "source": [
        "!git pull # Download updated project from Github.\n",
        "\n",
        "##### HANDLE DIFFS WHEN RUNNING IN COLAB #####\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "print(\"In Colab:\", IN_COLAB)\n",
        "import sys\n",
        "if (IN_COLAB):\n",
        "  sys.path.insert(1, '/content/src/')\n",
        "##### HANDLE DIFFS WHEN RUNNING IN COLAB #####\n",
        "\n",
        "######### EXTERNAL LIBRARIES #########\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "#NOTE: Good resource. -> https://www.tensorflow.org/tutorials/quickstart/advanced\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, UpSampling2D, MaxPool2D\n",
        "from tensorflow.keras import Model\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "import random\n",
        "from qmindcolors import cstr\n",
        "import cv2\n",
        "!pip install chumpy\n",
        "######### EXTERNAL LIBRARIES #########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Annotations\n",
        "\n",
        "Running this block will populate y_train and y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EMJu2pBWKJx8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading in training annotations\n",
            "\u001b[96mTraining annotations loaded in 3.5048351287841797 s\u001b[0m\n",
            "Loading in evaluation annotations\n",
            "\u001b[96mEvaluation annotations loaded in 0.1605689525604248 s\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "anno_train_path = os.path.join(\"data\", \"anno\", \"anno_training.pickle\") if IN_COLAB else \\\n",
        "    os.path.join(\"..\", \"RHD_small\", \"training\", \"anno_training.pickle\")\n",
        "anno_eval_path = os.path.join(\"data\", \"anno\", \"anno_evaluation.pickle\") if IN_COLAB else \\\n",
        "    os.path.join(\"..\", \"RHD_small\", \"evaluation\", \"anno_evaluation.pickle\")\n",
        "\n",
        "# NOTE: We note that the numbers 41258 and 2728 were retrieved directly from\n",
        "# https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html\n",
        "TRAIN_TOTAL_COUNT = 41258\n",
        "EVALUATION_TOTAL_COUNT = 2728\n",
        "\n",
        "y_train = np.zeros((TRAIN_TOTAL_COUNT, 21, 3))\n",
        "y_test = np.zeros((EVALUATION_TOTAL_COUNT, 21, 3))\n",
        "k_train = np.zeros((TRAIN_TOTAL_COUNT, 3, 3))\n",
        "k_test = np.zeros((EVALUATION_TOTAL_COUNT, 3, 3))\n",
        "y2_train = np.zeros((TRAIN_TOTAL_COUNT, 21, 2))\n",
        "y2_test = np.zeros((EVALUATION_TOTAL_COUNT, 21, 2))\n",
        "\n",
        "def load_anno(path, y, k, y2):\n",
        "  anno_all = []\n",
        "  count = 0\n",
        "  with open(path, 'rb') as f:\n",
        "    anno_all = pickle.load(f)\n",
        "  for key, value in anno_all.items():\n",
        "    kp_visible = (value['uv_vis'][:, 2] == 1)\n",
        "    case1 = np.sum(kp_visible[0:21])\n",
        "    case2 = np.sum(kp_visible[21:])\n",
        "    leftHand = case1 > 0\n",
        "    # NOTE: We note here that we are not checking if this training or evaluation example is valid.\n",
        "    # i.e. we want to densely store the annotations.\n",
        "    if(not leftHand):\n",
        "        y[count, :, :] = value['xyz'][21:42]\n",
        "        y2[count, :, :] = value['uv_vis'][:, :2][21:42]\n",
        "    else: \n",
        "        y[count, :, :] = value['xyz'][:21]\n",
        "        y2[count, :, :] = value['uv_vis'][:, :2][:21]\n",
        "\n",
        "    # Adjust the 3D keypoints to be at the center of the image.\n",
        "    annot_3D = y[count, :, :]\n",
        "    y[count, :, :] -= np.array([annot_3D[0][0], annot_3D[0][1], 0.0], dtype=np.float32)\n",
        "\n",
        "    k[count, :, :] = value['K']\n",
        "    count += 1\n",
        "\n",
        "print(\"Loading in training annotations\")\n",
        "time_start = time.time()\n",
        "load_anno(anno_train_path, y_train, k_train, y2_train)\n",
        "time_end = time.time()\n",
        "print(cstr(\"Training annotations loaded in {} s\".format(time_end - time_start)))\n",
        "print(\"Loading in evaluation annotations\")\n",
        "time_start = time.time()\n",
        "load_anno(anno_eval_path, y_test, k_test, y2_test)\n",
        "time_end = time.time()\n",
        "print(cstr(\"Evaluation annotations loaded in {} s\".format(time_end - time_start)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pebm8lODx0fu"
      },
      "source": [
        "# DATA LOADING\n",
        "\n",
        "IMAGE_SIZE, GRAYSCALE, and BATCH_SIZE should generally not be changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 224\n",
        "GRAYSCALE = False\n",
        "BATCH_SIZE = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7SDaXTQZuF0y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[96mDESIRED_BATCH_COUNT =\u001b[0m 16\n",
            "\u001b[96mDESIRED_BATCH_COUNT\u001b[0m is the batches per epoch to train w/\n",
            "\u001b[96mDESIRED_TEST_BATCH_COUNT =\u001b[0m 16\n",
            "\u001b[96mDESIRED_TEST_BATCH_COUNT\u001b[0m is the batches per epoch to test w/\n"
          ]
        }
      ],
      "source": [
        "IMAGE_CHANNELS = 1 if GRAYSCALE else 3\n",
        "\n",
        "# NOTE(Noah): gcs code will only work on the Colab. It works on either Ubuntu or macOS (no Windows support).\n",
        "# I attempted to install gcsfuse on my macOS machine, but it did not work.\n",
        "# gsfuse is beta software.\n",
        "if IN_COLAB:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  # we know that we are on an Ubuntu machine.\n",
        "  # Thus, installing gcsfuse will be done via the Ubuntu instructions.\n",
        "  # https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/installing.md#ubuntu-and-debian-latest-releases\n",
        "  !echo \"deb http://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "  !curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "  # -y in apt-get will assume \"yes\" as the answer to all prompts.\n",
        "  # -q in apt-get will make things \"quiet\" for us. Nice!\n",
        "  !sudo apt-get -y -q update\n",
        "  !sudo apt-get -y -q install gcsfuse\n",
        "  !mkdir -p data\n",
        "  !gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 shd_final data\n",
        "\n",
        "# NOTE(Noah): Stole this function from Stackoverflow :)\n",
        "def rgb2gray(rgb):\n",
        "  return np.expand_dims(np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140]), axis=2)\n",
        "    \n",
        "def resize(img, size):\n",
        "  return cv2.resize(img, dsize=(size, size), interpolation=cv2.INTER_CUBIC)\n",
        "  \n",
        "def download_image(root_dir, set, img_index):\n",
        "  file_path = os.path.join(root_dir, set, \"color\", \"{:05d}.png\".format(img_index))\n",
        "  image = imageio.imread(file_path)\n",
        "  _image = image.astype('float32')\n",
        "  if GRAYSCALE:\n",
        "    _image = rgb2gray(_image / 255)\n",
        "  else:\n",
        "    _image = _image / 255\n",
        "\n",
        "  annot_2D = y2_train[img_index]\n",
        "  if set == \"evaluation\":\n",
        "    annot_2D = y2_test[img_index]\n",
        "  pixel_trans = np.array([160,160]) - annot_2D[0]\n",
        "  x_shift = int(pixel_trans[0])\n",
        "  y_shift = int(pixel_trans[1])\n",
        "  _image = np.roll( _image, (y_shift, x_shift), axis=(0,1) )\n",
        "\n",
        "  # black out the regions we do not care about\n",
        "  if y_shift > 0:\n",
        "    _image = cv2.rectangle(_image, (0, 0), (320, y_shift), 0, -1)\n",
        "  else:\n",
        "    _image = cv2.rectangle(_image, (0, 320 + y_shift), (320, 320), 0, -1)\n",
        "\n",
        "  if x_shift > 0:\n",
        "    _image = cv2.rectangle(_image, (0, 0), (x_shift, 320), 0, -1)\n",
        "  else:\n",
        "    _image = cv2.rectangle(_image, (320 + x_shift, 0), (320, 320), 0, -1)\n",
        "\n",
        "  _image = resize(_image, IMAGE_SIZE)\n",
        "\n",
        "  return _image\n",
        "\n",
        "# TODO(Noah): Reimplement the code that sets up SH_RHD.\n",
        "gcs_path = 'data' if IN_COLAB else os.path.join(\"..\", \"SH_RHD\")\n",
        "train_list = os.listdir(os.path.join(gcs_path, \"training/color\"))\n",
        "eval_list = os.listdir(os.path.join(gcs_path, \"evaluation/color\"))\n",
        "\n",
        "# Below, we implement stochastic subsampling of the train and eval list so\n",
        "# that our model will train in a reasonable amount of time.\n",
        "DESIRED_BATCH_COUNT = min(16, len(train_list) // BATCH_SIZE)\n",
        "print(cstr(\"DESIRED_BATCH_COUNT =\"), DESIRED_BATCH_COUNT)\n",
        "print(cstr(\"DESIRED_BATCH_COUNT\"), \"is the batches per epoch to train w/\")\n",
        "DESIRED_TEST_BATCH_COUNT = min(16, len(eval_list) // BATCH_SIZE)\n",
        "print(cstr(\"DESIRED_TEST_BATCH_COUNT =\"), DESIRED_BATCH_COUNT)\n",
        "print(cstr(\"DESIRED_TEST_BATCH_COUNT\"), \"is the batches per epoch to test w/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSW4BpvZPIsb"
      },
      "source": [
        "# MODEL LOADING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JLws7Z0QPIsb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 288)               182880    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 288)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 288)               83232     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 288)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 58)                16762     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 282,874\n",
            "Trainable params: 282,874\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "MANO Differentiable Layer Loaded\n",
            "\u001b[96moutput_test =\u001b[0m [<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
            "array([[0.18377572, 0.11077617, 0.31240058, 0.        , 0.04905417,\n",
            "        0.        , 0.2761874 , 0.        , 0.18499321, 0.        ]],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(1, 48), dtype=float32, numpy=\n",
            "array([[0.30291492, 0.        , 0.        , 0.        , 0.27091745,\n",
            "        0.13602516, 0.        , 0.        , 0.29632658, 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.13734527,\n",
            "        0.00066157, 0.13665064, 0.        , 0.        , 0.        ,\n",
            "        0.        , 0.01632619, 0.16174822, 0.        , 0.24117535,\n",
            "        0.        , 0.01992277, 0.14143184, 0.07286463, 0.3852568 ,\n",
            "        0.        , 0.09915306, 0.        , 0.        , 0.26433453,\n",
            "        0.0579306 , 0.        , 0.        , 0.        , 0.34181526,\n",
            "        0.        , 0.        , 0.39961582, 0.13545851, 0.        ,\n",
            "        0.41731596, 0.16248032, 0.10638881]], dtype=float32)>, <tf.Tensor: shape=(32, 778, 3), dtype=float32, numpy=\n",
            "array([[[ 0.02670965, -0.06939972, -0.05910647],\n",
            "        [ 0.02793238, -0.04804192, -0.06010374],\n",
            "        [ 0.0458679 , -0.0537152 , -0.07106938],\n",
            "        ...,\n",
            "        [-0.05053744, -0.08107696, -0.06145367],\n",
            "        [-0.05275901, -0.09873918, -0.06279523],\n",
            "        [-0.03285543, -0.05549174, -0.05132588]],\n",
            "\n",
            "       [[ 0.02670965, -0.06939972, -0.05910647],\n",
            "        [ 0.02793238, -0.04804192, -0.06010374],\n",
            "        [ 0.0458679 , -0.0537152 , -0.07106938],\n",
            "        ...,\n",
            "        [-0.05053744, -0.08107696, -0.06145367],\n",
            "        [-0.05275901, -0.09873918, -0.06279523],\n",
            "        [-0.03285543, -0.05549174, -0.05132588]],\n",
            "\n",
            "       [[ 0.02670965, -0.06939972, -0.05910647],\n",
            "        [ 0.02793238, -0.04804192, -0.06010374],\n",
            "        [ 0.0458679 , -0.0537152 , -0.07106938],\n",
            "        ...,\n",
            "        [-0.05053744, -0.08107696, -0.06145367],\n",
            "        [-0.05275901, -0.09873918, -0.06279523],\n",
            "        [-0.03285543, -0.05549174, -0.05132588]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 0.02670965, -0.06939972, -0.05910647],\n",
            "        [ 0.02793238, -0.04804192, -0.06010374],\n",
            "        [ 0.0458679 , -0.0537152 , -0.07106938],\n",
            "        ...,\n",
            "        [-0.05053744, -0.08107696, -0.06145367],\n",
            "        [-0.05275901, -0.09873918, -0.06279523],\n",
            "        [-0.03285543, -0.05549174, -0.05132588]],\n",
            "\n",
            "       [[ 0.02670965, -0.06939972, -0.05910647],\n",
            "        [ 0.02793238, -0.04804192, -0.06010374],\n",
            "        [ 0.0458679 , -0.0537152 , -0.07106938],\n",
            "        ...,\n",
            "        [-0.05053744, -0.08107696, -0.06145367],\n",
            "        [-0.05275901, -0.09873918, -0.06279523],\n",
            "        [-0.03285543, -0.05549174, -0.05132588]],\n",
            "\n",
            "       [[ 0.02670965, -0.06939972, -0.05910647],\n",
            "        [ 0.02793238, -0.04804192, -0.06010374],\n",
            "        [ 0.0458679 , -0.0537152 , -0.07106938],\n",
            "        ...,\n",
            "        [-0.05053744, -0.08107696, -0.06145367],\n",
            "        [-0.05275901, -0.09873918, -0.06279523],\n",
            "        [-0.03285543, -0.05549174, -0.05132588]]], dtype=float32)>, <tf.Tensor: shape=(32, 21, 3), dtype=float32, numpy=\n",
            "array([[[-0.00172312, -0.00023022, -0.00016018],\n",
            "        [ 0.13709173, -0.05312868, -0.14324895],\n",
            "        [ 0.10536931, -0.06591424, -0.09592097],\n",
            "        ...,\n",
            "        [-0.11139122, -0.1580768 , -0.06849014],\n",
            "        [-0.09392544, -0.13470136, -0.05611979],\n",
            "        [-0.07348521, -0.10611522, -0.05056238]],\n",
            "\n",
            "       [[-0.00172312, -0.00023022, -0.00016018],\n",
            "        [ 0.13709173, -0.05312868, -0.14324895],\n",
            "        [ 0.10536931, -0.06591424, -0.09592097],\n",
            "        ...,\n",
            "        [-0.11139122, -0.1580768 , -0.06849014],\n",
            "        [-0.09392544, -0.13470136, -0.05611979],\n",
            "        [-0.07348521, -0.10611522, -0.05056238]],\n",
            "\n",
            "       [[-0.00172312, -0.00023022, -0.00016018],\n",
            "        [ 0.13709173, -0.05312868, -0.14324895],\n",
            "        [ 0.10536931, -0.06591424, -0.09592097],\n",
            "        ...,\n",
            "        [-0.11139122, -0.1580768 , -0.06849014],\n",
            "        [-0.09392544, -0.13470136, -0.05611979],\n",
            "        [-0.07348521, -0.10611522, -0.05056238]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-0.00172312, -0.00023022, -0.00016018],\n",
            "        [ 0.13709173, -0.05312868, -0.14324895],\n",
            "        [ 0.10536931, -0.06591424, -0.09592097],\n",
            "        ...,\n",
            "        [-0.11139122, -0.1580768 , -0.06849014],\n",
            "        [-0.09392544, -0.13470136, -0.05611979],\n",
            "        [-0.07348521, -0.10611522, -0.05056238]],\n",
            "\n",
            "       [[-0.00172312, -0.00023022, -0.00016018],\n",
            "        [ 0.13709173, -0.05312868, -0.14324895],\n",
            "        [ 0.10536931, -0.06591424, -0.09592097],\n",
            "        ...,\n",
            "        [-0.11139122, -0.1580768 , -0.06849014],\n",
            "        [-0.09392544, -0.13470136, -0.05611979],\n",
            "        [-0.07348521, -0.10611522, -0.05056238]],\n",
            "\n",
            "       [[-0.00172312, -0.00023022, -0.00016018],\n",
            "        [ 0.13709173, -0.05312868, -0.14324895],\n",
            "        [ 0.10536931, -0.06591424, -0.09592097],\n",
            "        ...,\n",
            "        [-0.11139122, -0.1580768 , -0.06849014],\n",
            "        [-0.09392544, -0.13470136, -0.05611979],\n",
            "        [-0.07348521, -0.10611522, -0.05056238]]], dtype=float32)>]\n",
            "MANO Differentiable Layer Loaded\n"
          ]
        }
      ],
      "source": [
        "MANO_DIR = os.path.join(\"data\", \"mano_v1_2\") if IN_COLAB else os.path.join(\"..\", \"mano_v1_2\")\n",
        "from mobilehand import MAKE_MOBILE_HAND\n",
        "from mobilehand_lfuncs import LOSS_3D\n",
        "MOBILE_HAND = MAKE_MOBILE_HAND(IMAGE_SIZE, IMAGE_CHANNELS, BATCH_SIZE, MANO_DIR)\n",
        "model = MOBILE_HAND\n",
        "\n",
        "### MODEL FORWARD PASS TEST ###\n",
        "input_test = tf.random.uniform(shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS))\n",
        "input_test = tf.cast(input_test, tf.float32)\n",
        "output_test = MOBILE_HAND((input_test, \n",
        "    np.repeat(np.array([[1]]), repeats=32), \n",
        "    tf.repeat(tf.constant([[0, 0, 0]]), repeats=32, axis=0) ))\n",
        "print(cstr(\"output_test =\"), output_test)\n",
        "### MODEL FORWARD PASS TEST ###\n",
        "\n",
        "from mobilehand_lfuncs import LOSS\n",
        "from mano_layer import MANO_Model\n",
        "_mpi_model = MANO_Model(MANO_DIR)\n",
        "# TODO(Noah): Expose U and L directly on our mobilehand implementation.\n",
        "U = _mpi_model.U\n",
        "L = _mpi_model.L\n",
        "loss_fn = lambda beta, pose, L, U, pred, gt : LOSS(beta, pose, L, U, pred, gt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3xZXThyyVta"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpOZpsk5rNev"
      },
      "outputs": [],
      "source": [
        "class StupidSimpleLossMetric():\n",
        "    def __init__(self):\n",
        "        self.losses = [] # empty python array \n",
        "    def __call__(self, loss):\n",
        "        self.losses.append(loss)\n",
        "    def result(self):\n",
        "        return sum(self.losses) / len(self.losses)\n",
        "    def reset_states(self):\n",
        "        self.losses = []\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam() # defaults should work just fine\n",
        "train_loss = StupidSimpleLossMetric()\n",
        "test_loss = StupidSimpleLossMetric()\n",
        "\n",
        "@tf.function\n",
        "def train_step(input, scale, z_depth, gt):\n",
        "    with tf.GradientTape() as tape:\n",
        "        beta, pose, mesh, keypoints = model((input, scale, z_depth))\n",
        "        #loss = loss_func(predictions, segmentation_masks)\n",
        "        #loss = np.dot(tf.reshape(segmentation_masks, [102400], tf.reshape(predictions, [102400])\n",
        "        #loss = loss_fn(keypoints, gt)\n",
        "        loss = loss_fn(beta, pose, L, U, keypoints, gt)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "    #train_accuracy(labels, predictions)\n",
        "  \n",
        "@tf.function\n",
        "def test_step(images, scale, z_depth, labels):\n",
        "  # training=False is only needed if there are layers with different\n",
        "  # behavior during training versus inference (e.g. Dropout).\n",
        "  beta, pose, mesh, keypoints = model((images, scale, z_depth), training=False)\n",
        "  return loss_fn(beta, pose, L, U, keypoints, labels)\n",
        "  #test_accuracy(labels, predictions)\n",
        "\n",
        "checkpoint_path = os.path.join(\"data\", \"checkpoints\") if IN_COLAB else os.path.join(\"..\", \"checkpoints/\")\n",
        "\n",
        "if not IN_COLAB:\n",
        "    from render_ckpt import render_checkpoint_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "\n",
        "The variable LAST_CHECKPOINT controls where our model training will start off from.\n",
        "Leave as -1 to \"start fresh\".\n",
        "OR adjust to any number, so long as there is a checkpoint saved for that number.\n",
        "\n",
        "You may also adjust EPOCHS as you wish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LAST_CHECKPOINT = -1\n",
        "EPOCHS = 10 # sure..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYZawQmCPIsf"
      },
      "outputs": [],
      "source": [
        "!mkdir $checkpoint_path\n",
        "\n",
        "random.shuffle(train_list)\n",
        "random.shuffle(eval_list)\n",
        "# numpy \"buckets\" that we will use to load things in.\n",
        "x_train = np.zeros( (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS) )\n",
        "x_test = np.zeros( (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS) ) \n",
        "\n",
        "LAST_CHECKPOINT = -1\n",
        "if (LAST_CHECKPOINT > -1):\n",
        "  file_path = os.path.join(checkpoint_path, \"cp-{:04d}.ckpt\".format(LAST_CHECKPOINT))\n",
        "  model.load_weights(file_path)\n",
        "  print(cstr(\"Loaded weights from {}\".format(file_path)))\n",
        "LAST_CHECKPOINT = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  print(\"Begin epoch\", epoch)\n",
        "  start = time.time()\n",
        "  train_loss.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  \n",
        "  y = np.zeros([BATCH_SIZE, 21, 3], dtype=np.float32)\n",
        "  \n",
        "  for i in range(DESIRED_BATCH_COUNT):\n",
        "    for j in range(BATCH_SIZE):\n",
        "      filename = train_list[j + i * BATCH_SIZE]\n",
        "      y_index = int(filename[0:5])\n",
        "      train_image = download_image(gcs_path, \"training\", y_index)\n",
        "      x_train[j,:,:,:] = train_image\n",
        "      y[j, :, :] = y_train[y_index]\n",
        "\n",
        "    x_train = x_train.astype('float32')\n",
        "    \n",
        "    # TODO(Noah): Do something with this constant value as it is used quite a bit thru the codebase.\n",
        "    scale = np.sqrt(np.sum(\n",
        "      np.square(y[:, 0] - y[:, 8]), axis=1, keepdims=True)) / 0.1537328322252615\n",
        "    z_depth = tf.constant(y[:, 0])\n",
        "\n",
        "    loss = train_step(x_train, scale, z_depth, y)\n",
        "    train_loss(loss.numpy())\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  print(\n",
        "    f'Epoch {epoch}, '\n",
        "    f'Time {end-start} s'\n",
        "    f'Loss: {train_loss.result()}, '\n",
        "    #f'Test Loss: {test_loss.result()}, '\n",
        "  )\n",
        "\n",
        "  # Save the model parameters\n",
        "  if (epoch % 1 == 0) or (epoch == EPOCHS - 1):\n",
        "    ckpt_index = LAST_CHECKPOINT + epoch\n",
        "    checkpoint_filepath = os.path.join(checkpoint_path, \"cp-{:04d}.ckpt\".format(ckpt_index))\n",
        "    model.save_weights(checkpoint_filepath)\n",
        "    print(cstr(\"Saved weights to {}\".format(checkpoint_filepath)))\n",
        "\n",
        "    if not IN_COLAB:\n",
        "      # Run the model on image 19 of the evaluation images.\n",
        "      test_img = 19\n",
        "      eval_image = download_image(gcs_path, \"evaluation\", test_img)\n",
        "      eval_image = eval_image.astype('float32')\n",
        "      annot = (y2_test[test_img], y_test[test_img], k_test[test_img])\n",
        "      render_checkpoint_image(checkpoint_path, ckpt_index, model, eval_image, annot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop to Test Overfitting\n",
        "\n",
        "Here we implement a training loop to test if we can overfit the model on just one image. We believe that this test will generally validate our model architecture and it's trainability. We have been having major issues with underfitting, and we belive that passing this overfit test is key to our success."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir $checkpoint_path\n",
        "\n",
        "last_checkpoint = -1\n",
        "if (last_checkpoint > -1):\n",
        "  file_path = os.path.join(checkpoint_path, \"cp-{:04d}.ckpt\".format(last_checkpoint))\n",
        "  model.load_weights(file_path)\n",
        "  print(cstr(\"Loaded weights from {}\".format(file_path)))\n",
        "\n",
        "# load the crap AND we only have to do it ONE time because we are simply trying to overfit\n",
        "# on the model.\n",
        "y = np.zeros([BATCH_SIZE, 21, 3], dtype=np.float32)\n",
        "x_train = np.zeros((BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS))\n",
        "selected_image = -1 # selected image\n",
        "for j in range(BATCH_SIZE):\n",
        "  filename = train_list[3]\n",
        "  selected_image = int(filename[0:5])\n",
        "  train_image = download_image(gcs_path, \"training\", selected_image)\n",
        "  x_train[j,:,:,:] = train_image\n",
        "  y[j, :, :] = y_train[selected_image]\n",
        "\n",
        "# TODO(Noah): We are having serious issues with modularity of code. But we need to iterate fast and\n",
        "# get something done.\n",
        "scale = np.sqrt(np.sum(\n",
        "  np.square(y[:, 0] - y[:, 8]), axis=1, keepdims=True)) / 0.1537328322252615\n",
        "z_depth = tf.constant(y[:, 0])\n",
        "\n",
        "EPOCHS = 10 # sure...\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  print(\"Begin epoch\", epoch)\n",
        "  start = time.time()\n",
        "  train_loss.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  \n",
        "  x_train = x_train.astype('float32')\n",
        "  loss = train_step(x_train, scale, z_depth, y)\n",
        "  train_loss(loss.numpy())\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  print(\n",
        "    f'Epoch {epoch}, '\n",
        "    f'Time {end-start} s'\n",
        "    f'Loss: {train_loss.result()}, '\n",
        "  )\n",
        "\n",
        "  # Save the model parameters for EACH EPOCH\n",
        "  checkpoint_filepath = os.path.join(checkpoint_path, \"cp-{:04d}.ckpt\".format(epoch))\n",
        "  model.save_weights(checkpoint_filepath)\n",
        "  print(cstr(\"Saved weights to {}\".format(checkpoint_filepath)))\n",
        "\n",
        "  if not IN_COLAB:\n",
        "    # Run the model on image 19 of the evaluation images.\n",
        "    test_img = selected_image\n",
        "    eval_image = download_image(gcs_path, \"training\", test_img)\n",
        "    eval_image = eval_image.astype('float32')\n",
        "    annot = (y2_train[test_img], y_train[test_img], k_train[test_img])\n",
        "    render_checkpoint_image(checkpoint_path, epoch, model, eval_image, annot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test \"render_checkpoint_image\"\n",
        "\n",
        "The block below will test the render_checkpoint_image subroutine. We load image 19 from the evaluation set of RHD as input into the subroutine. \n",
        "\n",
        "Note the template_override parameter of the function call. Setting this to true will ignore the evalution image and no forward pass will happen. Instead, the MANO template mesh will be rendered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMG = 26\n",
        "# Run the model on image 19 of the evaluation images.\n",
        "eval_image = download_image(gcs_path, \"training\", IMG)\n",
        "eval_image = eval_image.astype('float32')\n",
        "\n",
        "annot_3D = y_train[IMG]\n",
        "annot_2D = y2_train[IMG]\n",
        "annot_K = k_train[IMG]\n",
        "annot = (annot_2D, annot_3D, annot_K)\n",
        "\n",
        "render_checkpoint_image(checkpoint_path, 0, model, eval_image, annot, template_override=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MODEL EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "rhd_eval_dir = os.path.join(gcs_path, \"evaluation\", \"color\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from evaluation import time_model\n",
        "time_model(model, rhd_eval_dir, download_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "00011.png\n",
            "00019.png\n",
            "00020.png\n",
            "00023.png\n",
            "00027.png\n",
            "00034.png\n",
            "00038.png\n",
            "00040.png\n",
            "00042.png\n",
            "00048.png\n",
            "00049.png\n",
            "00053.png\n",
            "00054.png\n",
            "00058.png\n",
            "00060.png\n",
            "00063.png\n",
            "00064.png\n",
            "00071.png\n",
            "00072.png\n",
            "00077.png\n",
            "00080.png\n",
            "00086.png\n",
            "00088.png\n",
            "00092.png\n",
            "00093.png\n",
            "00094.png\n",
            "00095.png\n",
            "00100.png\n",
            "00103.png\n",
            "00106.png\n",
            "00114.png\n",
            "00116.png\n",
            "00117.png\n",
            "00118.png\n",
            "00122.png\n",
            "00123.png\n",
            "00124.png\n",
            "00130.png\n",
            "00145.png\n",
            "00153.png\n",
            "00156.png\n",
            "00162.png\n",
            "00163.png\n",
            "00164.png\n",
            "00165.png\n",
            "00176.png\n",
            "00181.png\n",
            "00184.png\n",
            "00188.png\n",
            "00190.png\n",
            "00192.png\n",
            "00193.png\n",
            "00201.png\n",
            "00011.png\n",
            "00019.png\n",
            "00020.png\n",
            "00023.png\n",
            "00027.png\n",
            "00034.png\n",
            "00038.png\n",
            "00040.png\n",
            "00042.png\n",
            "00048.png\n",
            "00049.png\n",
            "00053.png\n",
            "00054.png\n",
            "00058.png\n",
            "00060.png\n",
            "00063.png\n",
            "00064.png\n",
            "00071.png\n",
            "00072.png\n",
            "00077.png\n",
            "00080.png\n",
            "00086.png\n",
            "00088.png\n",
            "00092.png\n",
            "00093.png\n",
            "00094.png\n",
            "00095.png\n",
            "00100.png\n",
            "00103.png\n",
            "00106.png\n",
            "00114.png\n",
            "00116.png\n",
            "00117.png\n",
            "00118.png\n",
            "00122.png\n",
            "00123.png\n",
            "00124.png\n",
            "00130.png\n",
            "00145.png\n",
            "00153.png\n",
            "00156.png\n",
            "00162.png\n",
            "00163.png\n",
            "00164.png\n",
            "00165.png\n",
            "00176.png\n",
            "00181.png\n",
            "00184.png\n",
            "00188.png\n",
            "00190.png\n",
            "00192.png\n",
            "00193.png\n",
            "00201.png\n",
            "00011.png\n",
            "00019.png\n",
            "00020.png\n",
            "00023.png\n",
            "00027.png\n",
            "00034.png\n",
            "00038.png\n",
            "00040.png\n",
            "00042.png\n",
            "00048.png\n",
            "00049.png\n",
            "00053.png\n",
            "00054.png\n",
            "00058.png\n",
            "00060.png\n",
            "00063.png\n",
            "00064.png\n",
            "00071.png\n",
            "00072.png\n",
            "00077.png\n",
            "00080.png\n",
            "00086.png\n",
            "00088.png\n",
            "00092.png\n",
            "00093.png\n",
            "00094.png\n",
            "00095.png\n",
            "00100.png\n",
            "00103.png\n",
            "00106.png\n",
            "00114.png\n",
            "00116.png\n",
            "00117.png\n",
            "00118.png\n",
            "00122.png\n",
            "00123.png\n",
            "00124.png\n",
            "00130.png\n",
            "00145.png\n",
            "00153.png\n",
            "00156.png\n",
            "00162.png\n",
            "00163.png\n",
            "00164.png\n",
            "00165.png\n",
            "00176.png\n",
            "00181.png\n",
            "00184.png\n",
            "00188.png\n",
            "00190.png\n",
            "00192.png\n",
            "00193.png\n",
            "00201.png\n",
            "00011.png\n",
            "00019.png\n",
            "00020.png\n",
            "00023.png\n",
            "00027.png\n",
            "00034.png\n",
            "00038.png\n",
            "00040.png\n",
            "00042.png\n",
            "00048.png\n",
            "00049.png\n",
            "00053.png\n",
            "00054.png\n",
            "00058.png\n",
            "00060.png\n",
            "00063.png\n",
            "00064.png\n",
            "00071.png\n",
            "00072.png\n",
            "00077.png\n",
            "00080.png\n",
            "00086.png\n",
            "00088.png\n",
            "00092.png\n",
            "00093.png\n",
            "00094.png\n",
            "00095.png\n",
            "00100.png\n",
            "00103.png\n",
            "00106.png\n",
            "00114.png\n",
            "00116.png\n",
            "00117.png\n",
            "00118.png\n",
            "00122.png\n",
            "00123.png\n",
            "00124.png\n",
            "00130.png\n",
            "00145.png\n",
            "00153.png\n",
            "00156.png\n",
            "00162.png\n",
            "00163.png\n",
            "00164.png\n",
            "00165.png\n",
            "00176.png\n",
            "00181.png\n",
            "00184.png\n",
            "00188.png\n",
            "00190.png\n",
            "00192.png\n",
            "00193.png\n",
            "00201.png\n",
            "00011.png\n",
            "00019.png\n",
            "00020.png\n",
            "00023.png\n",
            "00027.png\n",
            "00034.png\n",
            "00038.png\n",
            "00040.png\n",
            "00042.png\n",
            "00048.png\n",
            "00049.png\n",
            "00053.png\n",
            "00054.png\n",
            "00058.png\n",
            "00060.png\n",
            "00063.png\n",
            "00064.png\n",
            "00071.png\n",
            "00072.png\n",
            "00077.png\n",
            "00080.png\n",
            "00086.png\n",
            "00088.png\n",
            "00092.png\n",
            "00093.png\n",
            "00094.png\n",
            "00095.png\n",
            "00100.png\n",
            "00103.png\n",
            "00106.png\n",
            "00114.png\n",
            "00116.png\n",
            "00117.png\n",
            "00118.png\n",
            "00122.png\n",
            "00123.png\n",
            "00124.png\n",
            "00130.png\n",
            "00145.png\n",
            "00153.png\n",
            "00156.png\n",
            "00162.png\n",
            "00163.png\n",
            "00164.png\n",
            "00165.png\n",
            "00176.png\n",
            "00181.png\n",
            "00184.png\n",
            "00188.png\n",
            "00190.png\n",
            "00192.png\n",
            "00193.png\n",
            "00201.png\n",
            "00011.png\n",
            "00019.png\n",
            "00020.png\n",
            "00023.png\n",
            "00027.png\n",
            "00034.png\n",
            "00038.png\n",
            "00040.png\n",
            "00042.png\n",
            "00048.png\n",
            "00049.png\n",
            "00053.png\n",
            "00054.png\n",
            "00058.png\n",
            "00060.png\n",
            "00063.png\n",
            "00064.png\n",
            "00071.png\n",
            "00072.png\n",
            "00077.png\n",
            "00080.png\n",
            "00086.png\n",
            "00088.png\n",
            "00092.png\n",
            "00093.png\n",
            "00094.png\n",
            "00095.png\n",
            "00100.png\n",
            "00103.png\n",
            "00106.png\n",
            "00114.png\n",
            "00116.png\n",
            "00117.png\n",
            "00118.png\n",
            "00122.png\n",
            "00123.png\n",
            "00124.png\n",
            "00130.png\n",
            "00145.png\n",
            "00153.png\n",
            "00156.png\n",
            "00162.png\n",
            "00163.png\n",
            "00164.png\n",
            "00165.png\n",
            "00176.png\n",
            "00181.png\n",
            "00184.png\n",
            "00188.png\n",
            "00190.png\n",
            "00192.png\n",
            "00193.png\n",
            "00201.png\n",
            "00011.png\n",
            "00019.png\n",
            "00020.png\n",
            "00023.png\n",
            "00027.png\n",
            "00034.png\n",
            "00038.png\n",
            "00040.png\n",
            "00042.png\n",
            "00048.png\n",
            "00049.png\n",
            "00053.png\n",
            "00054.png\n",
            "00058.png\n",
            "00060.png\n",
            "00063.png\n",
            "00064.png\n",
            "00071.png\n",
            "00072.png\n",
            "00077.png\n",
            "00080.png\n",
            "00086.png\n",
            "00088.png\n",
            "00092.png\n",
            "00093.png\n",
            "00094.png\n",
            "00095.png\n",
            "00100.png\n",
            "00103.png\n",
            "00106.png\n",
            "00114.png\n",
            "00116.png\n",
            "00117.png\n",
            "00118.png\n",
            "00122.png\n",
            "00123.png\n",
            "00124.png\n",
            "00130.png\n",
            "00145.png\n",
            "00153.png\n",
            "00156.png\n",
            "00162.png\n",
            "00163.png\n",
            "00164.png\n",
            "00165.png\n",
            "00176.png\n",
            "00181.png\n",
            "00184.png\n",
            "00188.png\n",
            "00190.png\n",
            "00192.png\n",
            "00193.png\n",
            "00201.png\n",
            "tf.Tensor([20. 25. 30. 35. 40. 45. 50.], shape=(7,), dtype=float32)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from evaluation import evaluate_model\n",
        "checkpoint_path = os.path.join('../','checkpoints', \"cp-{:04d}.ckpt\".format(9))\n",
        "\n",
        "model.load_weights(checkpoint_path)\n",
        "evaluate_model(model, rhd_eval_dir, download_image, y_test, gcs_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from evaluation import generate_loss_graph\n",
        "generate_loss_graph(checkpoint_path, model, \n",
        "    rhd_eval_dir, download_image, y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FGNffUKbxtVJ"
      ],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "HandTracking.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
