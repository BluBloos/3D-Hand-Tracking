{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BluBloos/3D-Hand-Tracking/blob/main/src/HandTracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytgQcZUUwswL"
      },
      "source": [
        "# SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFT4_Ezkg0cx"
      },
      "source": [
        "## Run ONLY when in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_ZBgm__TNlq"
      },
      "outputs": [],
      "source": [
        "!echo \"Initializing github repository\"\n",
        "!ls -la\n",
        "!rm -r .config/\n",
        "!rm -r sample_data/\n",
        "!git clone https://github.com/BluBloos/QMIND2021-2022/ ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7uiZv5Lg0cz"
      },
      "source": [
        "## Always Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee72KKdTPIsZ"
      },
      "outputs": [],
      "source": [
        "!git pull # Download updated project from Github.\n",
        "\n",
        "##### HANDLE DIFFS WHEN RUNNING IN COLAB #####\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "print(\"In Colab:\", IN_COLAB)\n",
        "import sys\n",
        "if (IN_COLAB):\n",
        "  sys.path.insert(1, '/content/src/')\n",
        "##### HANDLE DIFFS WHEN RUNNING IN COLAB #####\n",
        "\n",
        "######### EXTERNAL LIBRARIES #########\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "#NOTE: Good resource. -> https://www.tensorflow.org/tutorials/quickstart/advanced\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, UpSampling2D, MaxPool2D\n",
        "from tensorflow.keras import Model\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "import random\n",
        "from qmindcolors import cstr\n",
        "import cv2\n",
        "!pip install chumpy\n",
        "######### EXTERNAL LIBRARIES #########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pebm8lODx0fu"
      },
      "source": [
        "# DATA LOADING\n",
        "\n",
        "IMAGE_SIZE, GRAYSCALE, and BATCH_SIZE should generally not be changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 224\n",
        "GRAYSCALE = False\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SDaXTQZuF0y"
      },
      "outputs": [],
      "source": [
        "IMAGE_CHANNELS = 1 if GRAYSCALE else 3\n",
        "\n",
        "# NOTE(Noah): gcs code will only work on the Colab. It works on either Ubuntu or macOS (no Windows support).\n",
        "# I attempted to install gcsfuse on my macOS machine, but it did not work.\n",
        "# gsfuse is beta software.\n",
        "if IN_COLAB:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  # we know that we are on an Ubuntu machine.\n",
        "  # Thus, installing gcsfuse will be done via the Ubuntu instructions.\n",
        "  # https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/installing.md#ubuntu-and-debian-latest-releases\n",
        "  !echo \"deb http://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "  !curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "  # -y in apt-get will assume \"yes\" as the answer to all prompts.\n",
        "  # -q in apt-get will make things \"quiet\" for us. Nice!\n",
        "  !sudo apt-get -y -q update\n",
        "  !sudo apt-get -y -q install gcsfuse\n",
        "  !mkdir -p data\n",
        "  !gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 shd_final data\n",
        "\n",
        "# NOTE(Noah): Stole this function from Stackoverflow :)\n",
        "def rgb2gray(rgb):\n",
        "  return np.expand_dims(np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140]), axis=2)\n",
        "    \n",
        "def resize(img, size):\n",
        "  return cv2.resize(img, dsize=(size, size), interpolation=cv2.INTER_CUBIC)\n",
        "  \n",
        "def download_image(path):\n",
        "  image = imageio.imread(path)\n",
        "  _image = image.astype('float32')\n",
        "  if GRAYSCALE:\n",
        "      _image = rgb2gray(_image / 255)\n",
        "  else:\n",
        "      _image = _image / 255\n",
        "  _image = resize(_image, IMAGE_SIZE)\n",
        "  return _image\n",
        "\n",
        "# TODO(Noah): Reimplement the code that sets up SH_RHD.\n",
        "gcs_path = 'data' if IN_COLAB else os.path.join(\"..\", \"SH_RHD\")\n",
        "train_list = os.listdir(os.path.join(gcs_path, \"training/color\"))\n",
        "eval_list = os.listdir(os.path.join(gcs_path, \"evaluation/color\"))\n",
        "\n",
        "# Below, we implement stochastic subsampling of the train and eval list so\n",
        "# that our model will train in a reasonable amount of time.\n",
        "DESIRED_BATCH_COUNT = min(16, len(train_list) // BATCH_SIZE)\n",
        "print(cstr(\"DESIRED_BATCH_COUNT =\"), DESIRED_BATCH_COUNT)\n",
        "print(cstr(\"DESIRED_BATCH_COUNT\"), \"is the batches per epoch to train w/\")\n",
        "DESIRED_TEST_BATCH_COUNT = min(16, len(eval_list) // BATCH_SIZE)\n",
        "print(cstr(\"DESIRED_TEST_BATCH_COUNT =\"), DESIRED_BATCH_COUNT)\n",
        "print(cstr(\"DESIRED_TEST_BATCH_COUNT\"), \"is the batches per epoch to test w/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Annotations\n",
        "\n",
        "Running this block will populate y_train and y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMJu2pBWKJx8"
      },
      "outputs": [],
      "source": [
        "anno_train_path = os.path.join(\"data\", \"anno\", \"anno_training.pickle\") if IN_COLAB else \\\n",
        "    os.path.join(\"..\", \"RHD_small\", \"training\", \"anno_training.pickle\")\n",
        "anno_eval_path = os.path.join(\"data\", \"anno\", \"anno_evaluation.pickle\") if IN_COLAB else \\\n",
        "    os.path.join(\"..\", \"RHD_small\", \"evaluation\", \"anno_evaluation.pickle\")\n",
        "\n",
        "# NOTE: We note that the numbers 41258 and 2728 were retrieved directly from\n",
        "# https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html\n",
        "TRAIN_TOTAL_COUNT = 41258\n",
        "EVALUATION_TOTAL_COUNT = 2728\n",
        "\n",
        "y_train = np.zeros((TRAIN_TOTAL_COUNT, 21, 3))\n",
        "y_test = np.zeros((EVALUATION_TOTAL_COUNT, 21, 3))\n",
        "\n",
        "def load_anno(path, y):\n",
        "  anno_all = []\n",
        "  count = 0\n",
        "  with open(path, 'rb') as f:\n",
        "    anno_all = pickle.load(f)\n",
        "  for key, value in anno_all.items():\n",
        "    kp_visible = (value['uv_vis'][:, 2] == 1)\n",
        "    case1 = np.sum(kp_visible[0:21])\n",
        "    case2 = np.sum(kp_visible[21:])\n",
        "    leftHand = case1 > 0\n",
        "    # NOTE: We note here that we are not checking if this training or evaluation example is valid.\n",
        "    # i.e. we want to densely store the annotations.\n",
        "    if(not leftHand):\n",
        "        y[count, :, :] = value['xyz'][21:42]\n",
        "    else: \n",
        "        y[count, :, :] = value['xyz'][:21]\n",
        "    count += 1\n",
        "\n",
        "print(\"Loading in training annotations\")\n",
        "time_start = time.time()\n",
        "load_anno(anno_train_path, y_train)\n",
        "time_end = time.time()\n",
        "print(cstr(\"Training annotations loaded in {} s\".format(time_end - time_start)))\n",
        "print(\"Loading in evaluation annotations\")\n",
        "time_start = time.time()\n",
        "load_anno(anno_eval_path, y_test)\n",
        "time_end = time.time()\n",
        "print(cstr(\"Evaluation annotations loaded in {} s\".format(time_end - time_start)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSW4BpvZPIsb"
      },
      "source": [
        "# MODEL LOADING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLws7Z0QPIsb"
      },
      "outputs": [],
      "source": [
        "MANO_DIR = os.path.join(\"data\", \"mano_v1_2\") if IN_COLAB else os.path.join(\"..\", \"mano_v1_2\")\n",
        "from mobilehand import MAKE_MOBILE_HAND\n",
        "from mobilehand_lfuncs import LOSS_3D\n",
        "MOBILE_HAND = MAKE_MOBILE_HAND(IMAGE_SIZE, IMAGE_CHANNELS, BATCH_SIZE, MANO_DIR)\n",
        "model = MOBILE_HAND\n",
        "\n",
        "### MODEL FORWARD PASS TEST ###\n",
        "input_test = tf.random.uniform(shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS))\n",
        "input_test = tf.cast(input_test, tf.float32)\n",
        "output_test = MOBILE_HAND(input_test)\n",
        "print(cstr(\"output_test =\"), output_test)\n",
        "### MODEL FORWARD PASS TEST ###\n",
        "\n",
        "from mobilehand_lfuncs import LOSS\n",
        "from mano_layer import MANO_Model\n",
        "_mpi_model = MANO_Model(MANO_DIR)\n",
        "# TODO(Noah): Expose U and L directly on our mobilehand implementation.\n",
        "U = _mpi_model.U\n",
        "L = _mpi_model.L\n",
        "loss_fn = lambda beta, pose, L, U, pred, gt : LOSS(beta, pose, L, U, pred, gt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3xZXThyyVta"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mpOZpsk5rNev"
      },
      "outputs": [],
      "source": [
        "class StupidSimpleLossMetric():\n",
        "    def __init__(self):\n",
        "        self.losses = [] # empty python array \n",
        "    def __call__(self, loss):\n",
        "        self.losses.append(loss)\n",
        "    def result(self):\n",
        "        return sum(self.losses) / len(self.losses)\n",
        "    def reset_states(self):\n",
        "        self.losses = []\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam() # defaults should work just fine\n",
        "train_loss = StupidSimpleLossMetric()\n",
        "test_loss = StupidSimpleLossMetric()\n",
        "\n",
        "@tf.function\n",
        "def train_step(input, gt):\n",
        "    with tf.GradientTape() as tape:\n",
        "        beta, pose, mesh, keypoints = model(input)\n",
        "        #loss = loss_func(predictions, segmentation_masks)\n",
        "        #loss = np.dot(tf.reshape(segmentation_masks, [102400], tf.reshape(predictions, [102400])\n",
        "        #loss = loss_fn(keypoints, gt)\n",
        "        loss = loss_fn(beta, pose, L, U, keypoints, gt)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "    #train_accuracy(labels, predictions)\n",
        "  \n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  # training=False is only needed if there are layers with different\n",
        "  # behavior during training versus inference (e.g. Dropout).\n",
        "  beta, pose, mesh, keypoints = model(images, training=False)\n",
        "  return loss_fn(beta, pose, L, U, keypoints, labels)\n",
        "  #test_accuracy(labels, predictions)\n",
        "\n",
        "checkpoint_path = os.path.join(\"data\", \"checkpoints\") if IN_COLAB else os.path.join(\"..\", \"checkpoints/\")\n",
        "\n",
        "if not IN_COLAB:\n",
        "    from render_ckpt import render_checkpoint_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "\n",
        "The variable LAST_CHECKPOINT controls where our model training will start off from.\n",
        "Leave as -1 to \"start fresh\".\n",
        "OR adjust to any number, so long as there is a checkpoint saved for that number.\n",
        "\n",
        "You may also adjust EPOCHS as you wish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "LAST_CHECKPOINT = -1\n",
        "EPOCHS = 10 # sure..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYZawQmCPIsf"
      },
      "outputs": [],
      "source": [
        "!mkdir $checkpoint_path\n",
        "\n",
        "random.shuffle(train_list)\n",
        "random.shuffle(eval_list)\n",
        "# numpy \"buckets\" that we will use to load things in.\n",
        "x_train = np.zeros( (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS) )\n",
        "x_test = np.zeros( (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS) ) \n",
        "\n",
        "LAST_CHECKPOINT = -1\n",
        "if (LAST_CHECKPOINT > -1):\n",
        "  file_path = os.path.join(checkpoint_path, \"cp-{:04d}.ckpt\".format(LAST_CHECKPOINT))\n",
        "  model.load_weights(file_path)\n",
        "  print(cstr(\"Loaded weights from {}\".format(file_path)))\n",
        "LAST_CHECKPOINT = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  print(\"Begin epoch\", epoch)\n",
        "  start = time.time()\n",
        "  train_loss.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  \n",
        "  y = np.zeros([BATCH_SIZE, 21, 3], dtype=np.float32)\n",
        "  \n",
        "  for i in range(DESIRED_BATCH_COUNT):\n",
        "    for j in range(BATCH_SIZE):\n",
        "      filename = train_list[j + i * BATCH_SIZE]\n",
        "      train_image = download_image(os.path.join(gcs_path, \"training\", \"color\", filename))\n",
        "      x_train[j,:,:,:] = train_image\n",
        "      y_index = int(filename[0:5])\n",
        "      y[j, :, :] = y_train[y_index]\n",
        "    x_train = x_train.astype('float32')\n",
        "    loss = train_step(x_train, y)\n",
        "    train_loss(loss.numpy())\n",
        " \n",
        "  for i in range(DESIRED_TEST_BATCH_COUNT):\n",
        "    for j in range(BATCH_SIZE):\n",
        "      filename = eval_list[j + i * BATCH_SIZE]\n",
        "      eval_image = download_image(os.path.join(gcs_path, \"evaluation\", \"color\", filename))\n",
        "      x_test[j,:,:,:] = eval_image\n",
        "      y_index = int(filename[0:5])\n",
        "      y[j, :, :] = y_test[y_index]\n",
        "    x_test = x_test.astype('float32')\n",
        "    loss_test = test_step(x_test, y)\n",
        "    test_loss(loss.numpy())\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  print(\n",
        "    f'Epoch {epoch}, '\n",
        "    f'Time {end-start} s'\n",
        "    f'Loss: {train_loss.result()}, '\n",
        "    f'Test Loss: {test_loss.result()}, '\n",
        "  )\n",
        "\n",
        "  # Save the model parameters\n",
        "  if (epoch % 5 == 0) or (epoch == EPOCHS - 1):\n",
        "    ckpt_index = LAST_CHECKPOINT + epoch\n",
        "    checkpoint_filepath = os.path.join(checkpoint_path, \"cp-{:04d}.ckpt\".format(ckpt_index))\n",
        "    model.save_weights(checkpoint_filepath)\n",
        "    print(cstr(\"Saved weights to {}\".format(checkpoint_filepath)))\n",
        "\n",
        "    if not IN_COLAB:\n",
        "      # Run the model on image 19 of the evaluation images.\n",
        "      test_img = 19\n",
        "      eval_image = download_image(os.path.join(gcs_path, \"evaluation\", \"color\", \"000{}.png\".format(test_img)))\n",
        "      eval_image = eval_image.astype('float32')\n",
        "      render_checkpoint_image(checkpoint_path, ckpt_index, model, eval_image, y_test[test_img])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test \"render_checkpoint_image\"\n",
        "\n",
        "The block below will test the render_checkpoint_image subroutine. We load image 19 from the evaluation set of RHD as input into the subroutine. \n",
        "\n",
        "Note the template_override parameter of the function call. Setting this to true will ignore the evalution image and no forward pass will happen. Instead, the MANO template mesh will be rendered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the model on image 19 of the evaluation images.\n",
        "eval_image = download_image(os.path.join(gcs_path, \"evaluation\", \"color\", \"00019.png\"))\n",
        "eval_image = eval_image.astype('float32')\n",
        "annot = y_test[19]\n",
        "render_checkpoint_image(checkpoint_path, 0, model, eval_image, annot, template_override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MODEL EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "rhd_eval_dir = os.path.join(gcs_path, \"evaluation\", \"color\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from evaluation import time_model\n",
        "time_model(model, rhd_eval_dir, download_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from evaluation import evaluate_model\n",
        "evaluate_model(model, rhd_eval_dir, download_image, y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FGNffUKbxtVJ"
      ],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "HandTracking.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
