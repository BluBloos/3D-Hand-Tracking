<h1>
  Occlusion invariant, dense two-hand tracking using a monocular RGB input
</h1>

<img src="https://vrscout.com/wp-content/uploads/2019/10/Screen-Shot-2019-10-01-at-12.36.47-PM.png" />
<h2>
  People
</h2>
<div style="display: flex; flex-direction: row;">
  <span>Noah Cabral</span>
  <span>Daniel Stewart</span>
</div>
<h2>
  Abstract
</h2>
<p>
  The problem is to track the 3D shape and pose of two-hands through high hand-to-hand and hand-to-object contact, using a monocular RGB input.
  Many of the actions that we take with our hands involve occlusion. We might interlace our hands while thinking, we might pick up an object, we might wash our hands, etc. 
  It is natural for hands to be in consistent contact and occlusion. As such, it remains an essential problem, if not the problem in hand tracking research.
  Effectively addressing this problem will enable much more immersive experiences in both AR and VR applications, enabling new modes of experience for players. 
  A future where people can communicate in virtual settings via sign language is certainly an exciting one. 
  This research also has great potential to improve medical and military training simulations, improve complex two-hand gesture tracking for general computer vision tasks, 
  and improve the field of motion capture for digital media.
  The goal of the project is to combine the different monocular RGB models to produce just one model that can track the 3D mesh of two hands, 
  that is invariant to occlusions from external objects, and is invariant to occlusions produced by inter- and intra-hand interactions. 
  The motivation of this research is primarily an engineering one, where the goal is to develop an intelligent system that solves a real problem better than 
  all alternative approaches. 
</p>
<h2>
  Paper
</h2>
<h2>
  Data and Code
</h2>
<h2>
  BibTeX
</h2>
<h2>
  Video
</h2>
<h2>
  Acknowledgements
</h2>
<p>
  This work was supported by Queen's Machine Intelligence and Neuroevolution Design (QMIND).
</p>
